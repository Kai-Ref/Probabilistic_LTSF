{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9271dfea-5927-488d-968e-f3770ae43d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a321272a-ae04-40e2-88c7-58e3eed0f767",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_nested_dict(base_dict, key_path, value):\n",
    "    \"\"\"Recursively update a nested dictionary given a dot-separated key.\"\"\"\n",
    "    keys = key_path.split(\".\")\n",
    "    d = base_dict\n",
    "    for k in keys[:-1]:  # Traverse the dictionary\n",
    "        if k not in d:\n",
    "            d[k] = {}  # Create nested dictionaries if they don't exist\n",
    "        d = d[k]\n",
    "    d[keys[-1]] = value  # Set the final value\n",
    "\n",
    "DATASET = 'etth1'\n",
    "MODEL = 'patchtst'\n",
    "args={'config': f'config/ltsf/{DATASET}/{MODEL}.yaml', \n",
    "        'seed_everything':0,\n",
    "       'data.data_manager.init_args.path': './datasets',\n",
    "       'trainer.default_root_dir': './log_dir',\n",
    "       'data.data_manager.init_args.dataset':f'{DATASET}',\n",
    "       'data.data_manager.init_args.split_val': True,\n",
    "       'trainer.max_epochs':50,\n",
    "       'data.data_manager.init_args.context_length':96,\n",
    "       'data.data_manager.init_args.prediction_length':96}\n",
    "\n",
    "yaml_config_path = f'/home/kreffert/Probabilistic_LTSF/ProbTS/{args[\"config\"]}'\n",
    "with open(yaml_config_path, \"r\") as f:\n",
    "    yaml_config = yaml.safe_load(f)\n",
    "# Merge the CLI arguments into the YAML config\n",
    "for key, value in args.items():\n",
    "    if key == \"config\":  # Skip the config file path\n",
    "        continue\n",
    "    update_nested_dict(yaml_config, key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "39eaacbe-29c3-4e76-b445-884a332243cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Long-term Dataset: etth1\n",
      "val  pred_len: 96 : num_test_windows: 29\n",
      "test  pred_len: 96 : num_test_windows: 29\n",
      "Test context length: [96], prediction length: [96]\n",
      "Validation context length: [96], prediction length: [96]\n",
      "Training context length: [96], prediction lengths: [96]\n",
      "Test rolling length: 96\n",
      "Variable-specific normalization: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kreffert/Probabilistic_LTSF/ProbTS/probts/data/data_utils/get_datasets.py:138: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stamp['date'] = pd.to_datetime(df_stamp.date, format=data_format)\n"
     ]
    }
   ],
   "source": [
    "from probts.data import DataManager, ProbTSDataModule\n",
    "\n",
    "data_manager = DataManager(**yaml_config[\"data\"][\"data_manager\"][\"init_args\"])\n",
    "# data_manager.prepare_dataset()\n",
    "data_module = ProbTSDataModule(data_manager= data_manager, \n",
    "                               batch_size= yaml_config[\"data\"]['batch_size'],\n",
    "                                test_batch_size= yaml_config[\"data\"]['test_batch_size'],\n",
    "                                num_workers= yaml_config[\"data\"]['num_workers'])\n",
    "\n",
    "train_loader = data_module.train_dataloader\n",
    "val_loader = data_module.val_dataloader\n",
    "test_loader = data_module.test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c0841ce8-564a-4d10-9f62-3ce7a422a715",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_model_link_args = [\"scaler\",\n",
    "                            \"train_pred_len_list\", ]\n",
    "data_to_forecaster_link_args = [\n",
    "    \"target_dim\",\n",
    "    \"history_length\",\n",
    "    \"context_length\",\n",
    "    \"prediction_length\",\n",
    "    \"train_pred_len_list\", \n",
    "    \"lags_list\",\n",
    "    \"freq\",\n",
    "    \"time_feat_dim\",\n",
    "    \"global_mean\",\n",
    "    \"dataset\"\n",
    "]\n",
    "\n",
    "for arg in data_to_forecaster_link_args:\n",
    "    if arg in dir(data_manager):\n",
    "        yaml_config['model']['forecaster']['init_args'][arg]=getattr(data_manager, arg)\n",
    "\n",
    "for arg in data_to_model_link_args:\n",
    "    if arg in dir(data_manager):\n",
    "        yaml_config['model'][arg] = getattr(data_manager, arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5074dec5-1ccb-4ef8-9dcf-4b42f3fea634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling_weight_scheme: none\n"
     ]
    }
   ],
   "source": [
    "from probts.model.forecast_module import ProbTSForecastModule\n",
    "from probts.model.forecaster.point_forecaster import PatchTST\n",
    "forecaster = PatchTST(**yaml_config[\"model\"]['forecaster']['init_args'])\n",
    "path=\"/home/kreffert/Probabilistic_LTSF/ProbTS/log_dir/etth1_PatchTST_CTX96_PRED96_seed0/ckpt/epoch=30-val_CRPS=0.335570.ckpt\"\n",
    "model = ProbTSForecastModule(forecaster=forecaster, \n",
    "                             **{k: v for k, v in yaml_config[\"model\"].items() if k != 'forecaster'})\n",
    "# model = model.load_from_checkpoint(path, **{k: v for k, v in yaml_config[\"model\"].items() if k != 'forecaster'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c23ad9cd-0710-4fd4-97c5-9436717d32b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'probts.model.forecast_module.ProbTSForecastModule'>\n"
     ]
    }
   ],
   "source": [
    "print(type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "934bfc0f-19b0-4118-9f27-4e26ae8bba34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "`model` must be a `LightningModule` or `torch._dynamo.OptimizedModule`, got `ProbTSForecastModule`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 12\u001b[0m\n\u001b[1;32m      4\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[1;32m      5\u001b[0m     max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,            \u001b[38;5;66;03m# Specify the number of epochs\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m#gpus=1,                   # Use a GPU, set to `None` or `0` for CPU\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     precision\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,             \u001b[38;5;66;03m# Mixed precision training, optional\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m#accumulate_grad_batches=1 # Gradient accumulation, optional\u001b[39;00m\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Train the model using the fit method\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# Your validation DataLoader\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/probts/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:533\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit\u001b[39m(\n\u001b[1;32m    500\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    501\u001b[0m     model: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.LightningModule\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    505\u001b[0m     ckpt_path: Optional[_PATH] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    506\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    507\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Runs the full optimization routine.\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \n\u001b[1;32m    509\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    531\u001b[0m \n\u001b[1;32m    532\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 533\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43m_maybe_unwrap_optimized\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    534\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m_lightning_module \u001b[38;5;241m=\u001b[39m model\n\u001b[1;32m    535\u001b[0m     _verify_strategy_supports_compile(model, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy)\n",
      "File \u001b[0;32m~/miniconda3/envs/probts/lib/python3.10/site-packages/pytorch_lightning/utilities/compile.py:111\u001b[0m, in \u001b[0;36m_maybe_unwrap_optimized\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[1;32m    110\u001b[0m _check_mixed_imports(model)\n\u001b[0;32m--> 111\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`model` must be a `LightningModule` or `torch._dynamo.OptimizedModule`, got `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(model)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    113\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: `model` must be a `LightningModule` or `torch._dynamo.OptimizedModule`, got `ProbTSForecastModule`"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "\n",
    "# Initialize the PyTorch Lightning trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=10,            # Specify the number of epochs\n",
    "    #gpus=1,                   # Use a GPU, set to `None` or `0` for CPU\n",
    "    precision=16,             # Mixed precision training, optional\n",
    "    #accumulate_grad_batches=1 # Gradient accumulation, optional\n",
    ")\n",
    "\n",
    "# Train the model using the fit method\n",
    "trainer.fit(\n",
    "    model,   # Your validation DataLoader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06c1c16-18c4-4049-aa55-b47965eb4a88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "probts",
   "language": "python",
   "name": "probts"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
