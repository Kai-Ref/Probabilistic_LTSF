{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from gluonts.dataset.repository.datasets import get_dataset\n",
    "from gluonts.dataset.multivariate_grouper import MultivariateGrouper\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data_path = 'path/to/datasets/'\n",
    "save_path = Path(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import STL\n",
    "from tqdm import trange\n",
    "\n",
    "def measure_strength(df, dataset, win=0):\n",
    "    \"\"\"\n",
    "    Measures the strength of trend (F_t) and seasonality (F_s) in time series data.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The input data containing time series columns.\n",
    "    - dataset (str): The name of the dataset to identify frequency or specific configurations.\n",
    "    - win (int): Window size for decomposition; if 0, applies decomposition on the full time series.\n",
    "\n",
    "    Outputs:\n",
    "    Prints the average strength of trend and seasonality for the dataset.\n",
    "    \"\"\"\n",
    "    # Decompose the time series for each dimension\n",
    "    dim_list = ts_decompose(df, dataset, win=win)\n",
    "    \n",
    "    F_t_list = []  # List to store trend strength values\n",
    "    F_s_list = []  # List to store seasonality strength values\n",
    "    \n",
    "    for res in dim_list:\n",
    "        # Skip calculations if variance of the decomposed components is zero\n",
    "        if (res.trend + res.resid).var() == 0 or (res.seasonal + res.resid).var() == 0:\n",
    "            continue\n",
    "        \n",
    "        # Calculate trend strength (F_t)\n",
    "        F_t = max(0, 1 - (res.resid.var() / (res.trend + res.resid).var()))\n",
    "        F_t_list.append(F_t)\n",
    "        \n",
    "        # Calculate seasonality strength (F_s)\n",
    "        F_s = max(0, 1 - (res.resid.var() / (res.seasonal + res.resid).var()))\n",
    "        F_s_list.append(F_s)\n",
    "    \n",
    "    # Print summary of results\n",
    "    print('dataset: {dataset}, \\t win. size: {win},\\t Avg. F_t: {avg_ft:2.4f},\\t Avg. F_s: {avg_fs:2.4f}'.format(\n",
    "        dataset=dataset, win=win, avg_ft=np.mean(F_t_list), avg_fs=np.mean(F_s_list)\n",
    "    ))\n",
    "\n",
    "def ts_decompose(df, dataset, win=0):\n",
    "    \"\"\"\n",
    "    Decomposes time series data into trend, seasonal, and residual components.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The input data containing time series columns.\n",
    "    - dataset (str): The name of the dataset to identify frequency or specific configurations.\n",
    "    - win (int): Window size for decomposition; if 0, applies decomposition on the full time series.\n",
    "\n",
    "    Returns:\n",
    "    - dim_list (list): A list of decomposition results for each dimension of the time series.\n",
    "    \"\"\"\n",
    "    # Define frequency mapping for datasets\n",
    "    freq_dict = {\n",
    "        'ETT-small/ETTh1': 'H', 'ETT-small/ETTh2': 'H', 'ETT-small/ETTm1': 'T', 'ETT-small/ETTm2': 'T',\n",
    "        'electricity/electricity': 'H', 'exchange_rate/exchange_rate': 'B',\n",
    "        'illness/national_illness': 'W', 'traffic/traffic': 'H', 'weather/weather': 'T',\n",
    "        'exchange_rate_nips': 'B', 'solar_nips': 'H', 'electricity_nips': 'H',\n",
    "        'traffic_nips': 'H', 'wiki2000_nips': 'D'\n",
    "    }\n",
    "    \n",
    "    # Define minimum period mapping for datasets\n",
    "    min_dict = {\n",
    "        'ETT-small/ETTm1': (24 * 60) // 15, 'ETT-small/ETTm2': (24 * 60) // 15,\n",
    "        'weather/weather': (24 * 60) // 10\n",
    "    }\n",
    "\n",
    "    dim = len(df.iloc[0])  # Number of dimensions (columns) in the data\n",
    "    dim_list = []  # List to store decomposition results for each dimension\n",
    "    \n",
    "    for i in trange(dim):  # Iterate over each column in the dataset\n",
    "        if win == 0:\n",
    "            # Standardize the time series column\n",
    "            tmp_df = (df.iloc[:, i] - df.iloc[:, i].mean()) / (df.iloc[:, i].std())\n",
    "            \n",
    "            # Perform STL decomposition with appropriate frequency settings\n",
    "            if dataset in freq_dict and freq_dict[dataset] == 'T':\n",
    "                stl = STL(tmp_df.fillna(0), period=7, robust=True)\n",
    "            else:\n",
    "                stl = STL(tmp_df.fillna(0), robust=True)\n",
    "            \n",
    "            res = stl.fit()  # Fit the decomposition model\n",
    "            dim_list.append(res)  # Store the result\n",
    "        else:\n",
    "            # Perform windowed decomposition\n",
    "            right = win  # Initialize the right boundary of the window\n",
    "            while right < len(df.iloc[1:, i]):\n",
    "                tmp_df = df.iloc[right - win:right, i]  # Extract the windowed data\n",
    "                tmp_df = (tmp_df - tmp_df.mean()) / (tmp_df.std())  # Standardize the windowed data\n",
    "                \n",
    "                # Perform STL decomposition with appropriate frequency settings\n",
    "                if dataset in freq_dict and freq_dict[dataset] == 'T':\n",
    "                    stl = STL(tmp_df.fillna(0), period=7, robust=True)\n",
    "                else:\n",
    "                    stl = STL(tmp_df.fillna(0), robust=True)\n",
    "                \n",
    "                res = stl.fit()  # Fit the decomposition model\n",
    "                right += win  # Move the window forward\n",
    "                dim_list.append(res)  # Store the result\n",
    "        \n",
    "    return dim_list  # Return the list of decomposition results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import normaltest\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "from scipy.stats import norm\n",
    "\n",
    "def test_normal(df, dataset, win=0):\n",
    "    dim = len(df.iloc[0])\n",
    "    score_list = []\n",
    "    gaussian_count = 0\n",
    "    count = 0\n",
    "    for i in range(dim):\n",
    "        # z-score\n",
    "        # df.iloc[:,i]=(df.iloc[:,i]-df.iloc[:,i].mean())/(df.iloc[:,i].std())\n",
    "        value = df.iloc[:,i].dropna().values\n",
    "        if len(value) < 10:\n",
    "            continue\n",
    "        \n",
    "        right = win\n",
    "        pvalue = []\n",
    "        if win > 0:\n",
    "            while right < len(value):\n",
    "                res = normaltest(value[right-win:right])[1]\n",
    "                pvalue.append(res)\n",
    "                right += win\n",
    "            res = np.mean(pvalue)\n",
    "        else:\n",
    "            res = normaltest(value)[1]\n",
    "            # res = kstest(value, 'norm')[1]\n",
    "            if sum(value) == 0:\n",
    "                continue\n",
    "            \n",
    "        if res >= 0.05:\n",
    "            gaussian_count += 1\n",
    "        count += 1\n",
    "            \n",
    "        score_list.append(res)\n",
    "\n",
    "    \n",
    "    print(dataset, \" gaussian pvalue: \", str(np.mean(score_list)), '\\t gaussian ratio: ', str(gaussian_count/count))\n",
    "\n",
    "\n",
    "def JS_divergence(p,q):\n",
    "    M=(p+q)/2\n",
    "    return 0.5*scipy.stats.entropy(p, M, base=2)+0.5*scipy.stats.entropy(q, M, base=2)\n",
    "\n",
    "def JS_div(arr1,arr2,num_bins):\n",
    "    max0 = max(np.max(arr1),np.max(arr2))\n",
    "    min0 = min(np.min(arr1),np.min(arr2))\n",
    "    bins = np.linspace(min0-1e-4, max0-1e-4, num=num_bins)\n",
    "    \n",
    "    PDF1 = pd.cut(arr1,bins,duplicates='drop').value_counts()\n",
    "    PDF2 = pd.cut(arr2,bins, duplicates='drop').value_counts()\n",
    "    \n",
    "    if sum(PDF1) > 0 and sum(PDF2) > 0:\n",
    "        PDF1 = PDF1 / len(arr1)\n",
    "        PDF2 = PDF2 / len(arr2)\n",
    "        return JS_divergence(PDF1.values,PDF2.values)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def cal_JS_divergence(df, dataset, win=0):\n",
    "    \n",
    "    dim = len(df.iloc[0])\n",
    "    js_list = []\n",
    "    for i in range(1, dim):\n",
    "        \n",
    "        # z-score\n",
    "        global_mu = df.iloc[:,i].mean()\n",
    "        global_std = df.iloc[:,i].std()\n",
    "        df.iloc[:,i]=(df.iloc[:,i]-global_mu) / global_std\n",
    "        value = df.iloc[:,i].dropna().values\n",
    "        \n",
    "        if sum(value) == 0:\n",
    "            continue\n",
    "        \n",
    "        right = win\n",
    "        dim_list = []\n",
    "        if win > 0:\n",
    "            while right < len(value):\n",
    "                tmp_value = value[right-win:right]\n",
    "                mu = tmp_value.mean()\n",
    "                std = tmp_value.std()\n",
    "\n",
    "                norm_dist = norm.rvs(loc=mu, scale=std, size=len(tmp_value))\n",
    "                res = JS_div(tmp_value,norm_dist,num_bins=20)\n",
    "                if res is not None:\n",
    "                    dim_list.append(res)\n",
    "                right += win\n",
    "                \n",
    "            js_div = np.mean(dim_list)\n",
    "\n",
    "        else:\n",
    "            norm_dist = norm.rvs(loc=global_mu, scale=global_std, size=len(value))\n",
    "            js_div = JS_div(value,norm_dist,num_bins=20)\n",
    "        \n",
    "        if js_div is not None:\n",
    "            js_list.append(js_div)\n",
    "        \n",
    "    print(\"window size: \", win, \"\\t dataset: \", dataset, \"\\t JS DIV avg: \", str(np.mean(js_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long-term Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_data(filename, dataset):\n",
    "    \"\"\"\n",
    "    Loads time series data from a CSV file and processes it based on dataset-specific requirements.\n",
    "\n",
    "    Parameters:\n",
    "    - filename (str): Path to the directory containing the CSV file.\n",
    "    - dataset (str): Name of the dataset to be loaded, used for specific handling.\n",
    "\n",
    "    Returns:\n",
    "    - df (pd.DataFrame): Processed DataFrame with time series data, indexed by date.\n",
    "    \"\"\"\n",
    "    # Dictionary to map dataset names to their respective data frequency\n",
    "    freq_dict = {\n",
    "        'ETT-small/ETTh1': 'H', 'ETT-small/ETTh2': 'H', 'ETT-small/ETTm1': 'T', 'ETT-small/ETTm2': 'T',\n",
    "        'electricity/electricity': 'H', 'exchange_rate/exchange_rate': 'D',\n",
    "        'illness/national_illness': 'D', 'traffic/traffic': 'H', 'weather/weather': 'T'\n",
    "    }\n",
    "\n",
    "    # Special handling for 'caiso' dataset\n",
    "    if 'caiso' in dataset:\n",
    "        # Load the dataset and convert the 'Date' column to datetime\n",
    "        data = pd.read_csv(filename + dataset + '.csv')\n",
    "        data['Date'] = data['Date'].astype('datetime64[ns]')\n",
    "        \n",
    "        # Names of zones in the dataset\n",
    "        names = ['PGE', 'SCE', 'SDGE', 'VEA', 'CA ISO', 'PACE', 'PACW', 'NEVP', 'AZPS', 'PSEI']\n",
    "        \n",
    "        # Create a DataFrame with a complete hourly date range\n",
    "        df = pd.DataFrame(pd.date_range('20130101', '20210630', freq='H')[:-1], columns=['Date'])\n",
    "        \n",
    "        # Process each zone's data and merge into a single DataFrame\n",
    "        for name in names:\n",
    "            current_df = (\n",
    "                data[data['zone'] == name]\n",
    "                .drop_duplicates(subset='Date', keep='last')  # Remove duplicate entries, keeping the last\n",
    "                .rename(columns={'load': name})  # Rename 'load' column to the zone name\n",
    "                .drop(columns=['zone'])  # Drop the 'zone' column\n",
    "            )\n",
    "            df = df.merge(current_df, on='Date', how='outer')  # Merge with the main DataFrame\n",
    "        \n",
    "        # Rename the 'Date' column to 'date'\n",
    "        df = df.rename(columns={'Date': 'date'})\n",
    "    elif 'nordpool' in dataset:\n",
    "        # Special handling for 'nordpool' dataset: Parse the 'Time' column as datetime\n",
    "        df = pd.read_csv(filename + dataset + '.csv', parse_dates=['Time'])\n",
    "        df = df.rename(columns={'Time': 'date'})  # Rename the 'Time' column to 'date'\n",
    "    else:\n",
    "        # General case: Load the dataset as-is\n",
    "        df = pd.read_csv(filename + dataset + '.csv')\n",
    "    \n",
    "    # Convert the 'date' column to datetime format and set it as the index\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df = df.set_index('date')\n",
    "\n",
    "    # Drop the first column (usually an index column or non-relevant column)\n",
    "    df = df.iloc[:, 1:]\n",
    "    \n",
    "    return df  # Return the processed DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:10<00:00,  1.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: ETT-small/ETTh1, \t win. size: 0,\t Avg. F_t: 0.7728,\t Avg. F_s: 0.4772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = 'ETT-small/ETTh1' # 'exchange_rate/exchange_rate'\n",
    "win_len = 0\n",
    "df = load_csv_data(data_path, dataset)\n",
    "measure_strength(df, dataset, win=win_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window size:  336 \t dataset:  ETT-small/ETTh1 \t JS DIV avg:  0.0719988819816385\n"
     ]
    }
   ],
   "source": [
    "dataset = 'ETT-small/ETTh1' # 'exchange_rate/exchange_rate'\n",
    "win_len = 336\n",
    "df = load_csv_data(data_path, dataset)\n",
    "cal_JS_divergence(df, dataset, win=win_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Short-term Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_prob_data(dataset, win=0):\n",
    "    freq_dict = {'exchange_rate_nips':'B','solar_nips':'H','electricity_nips':'H','traffic_nips':'H', 'wiki2000_nips':'D'}\n",
    "    \n",
    "    idx = 0\n",
    "    dataname = dataset\n",
    "    dataset = get_dataset(dataset, path=save_path, regenerate=False)\n",
    "    dim = int(dataset.metadata.feat_static_cat[0].cardinality)\n",
    "    train_grouper = MultivariateGrouper(max_target_dim=dim)\n",
    "    dataset_train = train_grouper(dataset.train)\n",
    "    data = list(dataset_train)[0]['target']\n",
    "    start_date = dataset_train[0]['start'].to_timestamp()\n",
    "    \n",
    "    # multi\n",
    "    idx = [i for i in range(dim)]\n",
    "\n",
    "    data = data.transpose(1,0)\n",
    "    df = pd.DataFrame(data,columns=idx,dtype=float)\n",
    "\n",
    "    df['date'] = pd.date_range(start_date,periods=len(data),freq=freq_dict[dataname]) \n",
    "    df = df.set_index('date')\n",
    "        \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/v-zhangjiaw/miniconda3/envs/probts/lib/python3.10/site-packages/gluonts/dataset/common.py:263: FutureWarning: Period with BDay freq is deprecated and will be removed in a future version. Use a DatetimeIndex with BDay freq instead.\n",
      "  return pd.Period(val, freq)\n",
      "/home/v-zhangjiaw/miniconda3/envs/probts/lib/python3.10/site-packages/gluonts/dataset/multivariate_grouper.py:114: FutureWarning: Period with BDay freq is deprecated and will be removed in a future version. Use a DatetimeIndex with BDay freq instead.\n",
      "  timestamp + len(data[FieldName.TARGET]) - 1,\n",
      "/home/v-zhangjiaw/miniconda3/envs/probts/lib/python3.10/site-packages/gluonts/dataset/multivariate_grouper.py:243: FutureWarning: Period with BDay freq is deprecated and will be removed in a future version. Use a DatetimeIndex with BDay freq instead.\n",
      "  index=pd.period_range(\n",
      "/home/v-zhangjiaw/miniconda3/envs/probts/lib/python3.10/site-packages/gluonts/dataset/multivariate_grouper.py:243: FutureWarning: PeriodDtype[B] is deprecated and will be removed in a future version. Use a DatetimeIndex with freq='B' instead\n",
      "  index=pd.period_range(\n",
      "/home/v-zhangjiaw/miniconda3/envs/probts/lib/python3.10/site-packages/gluonts/dataset/multivariate_grouper.py:188: FutureWarning: Period with BDay freq is deprecated and will be removed in a future version. Use a DatetimeIndex with BDay freq instead.\n",
      "  pd.period_range(\n",
      "/home/v-zhangjiaw/miniconda3/envs/probts/lib/python3.10/site-packages/gluonts/dataset/multivariate_grouper.py:188: FutureWarning: PeriodDtype[B] is deprecated and will be removed in a future version. Use a DatetimeIndex with freq='B' instead\n",
      "  pd.period_range(\n",
      "/tmp/ipykernel_1399510/2105741496.py:11: FutureWarning: Period with BDay freq is deprecated and will be removed in a future version. Use a DatetimeIndex with BDay freq instead.\n",
      "  start_date = dataset_train[0]['start'].to_timestamp()\n",
      "100%|██████████| 8/8 [00:01<00:00,  5.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: exchange_rate_nips, \t win. size: 0,\t Avg. F_t: 0.9982,\t Avg. F_s: 0.1256\n",
      "window size:  30 \t dataset:  exchange_rate_nips \t JS DIV avg:  0.2964380648448922\n"
     ]
    }
   ],
   "source": [
    "# \"exchange_rate_nips\", \"solar_nips\", \"electricity_nips\", \"traffic_nips\", \"taxi_30min\", \"wiki2000_nips\"\n",
    "dataset = \"exchange_rate_nips\"\n",
    "df = load_prob_data(dataset, win=0)\n",
    "\n",
    "measure_strength(df, dataset, win=0)\n",
    "cal_JS_divergence(df, dataset, win=30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "probts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
