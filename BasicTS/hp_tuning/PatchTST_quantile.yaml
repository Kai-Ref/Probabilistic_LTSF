method: bayes  # You can also use 'random' here
metric:
  name: val/loss
  goal: minimize
parameters:
  MODEL.NAME: 
    values: ['PatchTST']
  DATA_NAME:
    values: ['ETTh1']
  OUTPUT_LEN:
    values: [720]
  RESCALE:
    values: [True] #, False]
  NORM_EACH_CHANNEL:
    values: [True, False]
  # INPUT_LEN:
  #   values: []

  # MODEL params
  MODEL.PARAM.e_layers:
    values: [2, 3, 4, 5, 7, 10]
  MODEL.PARAM.n_heads:
    values: [2, 4, 8]
  MODEL.PARAM.d_model:
    values: [8, 16, 32, 64, 128]
  MODEL.PARAM.d_ff:
    values: [8, 16, 32, 64, 128]
  MODEL.PARAM.dropout:
    min: 0.01
    max: 0.4
    distribution: uniform 
  MODEL.PARAM.fc_dropout:
    min: 0.01
    max: 0.4
    distribution: uniform
  MODEL.PARAM.head_dropout:
    min: 0.01
    max: 0.4
    distribution: uniform
  MODEL.PARAM.attn_dropout:
    min: 0.01
    max: 0.4
    distribution: uniform
  MODEL.PARAM.padding_patch:
    values: ['None', 'end']
  MODEL.PARAM.patch_len:
    values: [2, 4, 8, 16, 32, 64, 128]
  MODEL.PARAM.stride:
    values: [2, 4, 8, 16, 32, 64, 128]
  MODEL.PARAM.individual:
    values: [0, 1]
  MODEL.PARAM.revin:
    values: [0, 1]
  MODEL.PARAM.affin:
    values: [0, 1]
  MODEL.PARAM.subtract_last:
    values: [0, 1]
  MODEL.PARAM.decomposition:
    values: [0, 1]
  MODEL.PARAM.kernel_size:
    values: [3, 7, 13, 25, 49]
  MODEL.PARAM.pe:
    values: ['normal', 'zeros', 'zero', uniform', 'lin1d', 'exp1d', 'lin2d', 'exp2d', 'sincos', 'None']
  MODEL.PARAM.learn_pe:
    values: [True]
  MODEL.PARAM.norm:
    values: ['LayerNorm', 'BatchNorm']
  MODEL.PARAM.act:
    values: ['gelu', 'relu']
  MODEL.PARAM.pre_norm:
    values: [True, False]
  MODEL.PARAM.head_type:
    values: ['probabilistic']
  MODEL.PARAM.distribution_type:
    values: ['quantile', "i_quantile"] #['gaussian', 'laplace', 'student_t'] #, 'flow'] # flow 
    # values: ['m_lr_gaussian']
  MODEL.PARAM.prob_args.quantiles:
    values: [[0.005, 0.025, 0.165, 0.25, 0.5, 0.75, 0.835, 0.975, 0.995]]
  MODEL.PARAM.prob_args.num_layers:
    values: [2, 3, 4, 5, 7, 10]
  MODEL.PARAM.prob_args.quantile_embed_dim: # note this affects
    values: [8, 16, 32, 64, 128]
  MODEL.PARAM.prob_args.decoding:
    values: ["concat", "hadamard"]
  MODEL.PARAM.prob_args.cos_embedding_dim:
    values: [8, 16, 32, 64, 128]
    
  # Basic Params
  NUM_EPOCHS:
    values: [100]
  USE_WANDB:
    values: [True]

  # SCALER 
  SCALER.TYPE:
    values: [ZScoreScaler, MinMaxScaler, None]

  # OPTIMIZER
  TRAIN.OPTIM.TYPE:
    values: [Adam]
  TRAIN.OPTIM.PARAM.lr:
    min: 2.5e-4
    max: 2.5e-2
    distribution: uniform  
  TRAIN.OPTIM.PARAM.weight_decay:
    min: 1.0e-5
    max: 1.0e-3
    distribution: uniform 
  
  # SCHEDULER
  TRAIN.LR_SCHEDULER.TYPE:
    values: [MultiStepLR] 
  TRAIN.LR_SCHEDULER.PARAM.gamma:
    min: 0.01
    max: 0.7
    distribution: uniform 
  TRAIN.DATA.BATCH_SIZE:
    values: [16, 32, 64, 128]
  TRAIN.RESUME_TRAINING:
    values: [False]  
  TRAIN.EARLY_STOPPING_PATIENCE: 
    values: [5]   
early_terminate:
  type: hyperband
  min_iter: 2
  eta: 3