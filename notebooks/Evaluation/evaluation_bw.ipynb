{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df69e24e-2162-4989-9bf0-b68682d6b1a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-02 18:23:53.683016: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-02 18:23:53.696307: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1751473433.710994 1350581 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1751473433.715652 1350581 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-07-02 18:23:53.732411: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-02 18:24:16,717 - easytorch-env - INFO - Use devices 0.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "prefix = \"/home/kreffert/\"\n",
    "prefix = \"/pfs/data6/home/ma/ma_ma/ma_kreffert/\" \n",
    "os.chdir(f'{prefix}Probabilistic_LTSF/BasicTS/')\n",
    "from basicts.metrics import masked_mae, masked_mse, nll_loss, crps, Evaluator, quantile_loss, empirical_crps\n",
    "from easytorch.device import set_device_type\n",
    "from easytorch.utils import get_logger, set_visible_devices\n",
    "# set the device type (CPU, GPU, or MLU)\n",
    "device_type ='gpu'\n",
    "gpus = '0'\n",
    "set_device_type(device_type)\n",
    "set_visible_devices(gpus)\n",
    "from easydict import EasyDict\n",
    "from tqdm import tqdm\n",
    "\n",
    "# extract the paths to the configs and weights\n",
    "import yaml\n",
    "# /home/kreffert/Probabilistic_LTSF/notebooks/Final plots/weights.yaml\n",
    "with open(f'{prefix}Probabilistic_LTSF/notebooks/Final plots/weights.yaml', 'r') as file:\n",
    "    _configs = yaml.safe_load(file)\n",
    "    \n",
    "def reconstruct_paths(\n",
    "    simplified_dict,\n",
    "    _dataset=['ETTh1', 'ETTm1'],\n",
    "    _models=['DLinear', 'PatchTST', 'DeepAR'],\n",
    "    _dists=['q', 'iq', 'u', 'm'],\n",
    "    _seeds=[0, 1, 2, 3, 4],\n",
    "    _model_dist_map=None  # optional: {'DLinear': ['m'], 'DeepAR': ['u']}\n",
    "):\n",
    "    base_path = \"final_weights/\"\n",
    "    dist_mapping = {\"iq\": \"i_quantile\", \"u\": \"univariate\", \"m\": \"multivariate\", \"q\": \"quantile\"}\n",
    "    filtered_dict = {}\n",
    "\n",
    "    for dataset, models in simplified_dict.items():\n",
    "        if dataset not in _dataset:\n",
    "            continue\n",
    "        for model, dists in models.items():\n",
    "            if _model_dist_map:\n",
    "                if model not in _model_dist_map:\n",
    "                    continue\n",
    "            elif model not in _models:\n",
    "                continue\n",
    "\n",
    "            allowed_dists = _model_dist_map[model] if _model_dist_map else _dists\n",
    "\n",
    "            for dist, seeds in dists.items():\n",
    "                if dist not in allowed_dists:\n",
    "                    continue\n",
    "                _cfg = f\"{dataset}_prob_quantile.py\" if dist in [\"q\", \"iq\"] else f\"{dataset}_prob.py\"\n",
    "                _ckpt = \"_best_val_QL.pt\" if dist in [\"q\", \"iq\"] else \"_best_val_NLL.pt\"\n",
    "                mapped_dist = dist_mapping.get(dist, dist)\n",
    "\n",
    "                for seed, path_suffix in seeds.items():\n",
    "                    if seed not in _seeds or path_suffix is None:\n",
    "                        continue\n",
    "                    prefix = f\"{base_path}{dataset}/{model}/{mapped_dist}/{seed}/{path_suffix}\"\n",
    "                    ckpt_path = f\"{prefix}/{model}{_ckpt}\"\n",
    "                    if os.path.isfile(ckpt_path):\n",
    "                        filtered_dict.setdefault(dataset, {}).setdefault(model, {}).setdefault(dist, {})[seed] = {\n",
    "                            'cfg': f\"{prefix}/{_cfg}\",\n",
    "                            'ckpt': f\"{prefix}/{model}{_ckpt}\"\n",
    "                        }\n",
    "                    # else:\n",
    "                    #     print(f\"{ckpt_path} not found.\")\n",
    "\n",
    "    return filtered_dict\n",
    "\n",
    "\n",
    "\n",
    "_model_dist_map={'DLinear': ['m'], 'DeepAR': ['u']}\n",
    "_models = ['DeepAR', 'DLinear'] #full_dict[dataset][model][dist][random_state]\n",
    "_dataset = ['ETTh1']\n",
    "_dists = ['u', 'm', 'q', 'iq']\n",
    "_seeds = [2]\n",
    "# _configs = reconstruct_paths(_configs, _dataset=_dataset, _models=_models, _dists=_dists, _seeds=_seeds, _model_dist_map=_model_dist_map)\n",
    "_configs = reconstruct_paths(_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29fab0d2-ccf4-4088-94f5-b42df5112832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ETTh1': {'DLinear': {'iq': {2: {'cfg': 'final_weights/ETTh1/DLinear/i_quantile/2/7e59031295d7db4c807d3b5f9db7bc80/ETTh1_prob_quantile.py', 'ckpt': 'final_weights/ETTh1/DLinear/i_quantile/2/7e59031295d7db4c807d3b5f9db7bc80/DLinear_best_val_QL.pt'}}, 'q': {2: {'cfg': 'final_weights/ETTh1/DLinear/quantile/2/a7f10e8c211a68a5aeb51fbaec3c28a5/ETTh1_prob_quantile.py', 'ckpt': 'final_weights/ETTh1/DLinear/quantile/2/a7f10e8c211a68a5aeb51fbaec3c28a5/DLinear_best_val_QL.pt'}}, 'm': {1: {'cfg': 'final_weights/ETTh1/DLinear/multivariate/1/aa5d43538e1d49f3c5367e942fdf55cb/ETTh1_prob.py', 'ckpt': 'final_weights/ETTh1/DLinear/multivariate/1/aa5d43538e1d49f3c5367e942fdf55cb/DLinear_best_val_NLL.pt'}, 2: {'cfg': 'final_weights/ETTh1/DLinear/multivariate/2/ab8ce283463444e38a4ab2c8b7fd3d83/ETTh1_prob.py', 'ckpt': 'final_weights/ETTh1/DLinear/multivariate/2/ab8ce283463444e38a4ab2c8b7fd3d83/DLinear_best_val_NLL.pt'}}}, 'DeepAR': {'iq': {2: {'cfg': 'final_weights/ETTh1/DeepAR/i_quantile/2/10b4108b626a9d27fad40eddcff3715a/ETTh1_prob_quantile.py', 'ckpt': 'final_weights/ETTh1/DeepAR/i_quantile/2/10b4108b626a9d27fad40eddcff3715a/DeepAR_best_val_QL.pt'}}, 'q': {2: {'cfg': 'final_weights/ETTh1/DeepAR/quantile/2/2ad27c696897fc216064f719a8bea178/ETTh1_prob_quantile.py', 'ckpt': 'final_weights/ETTh1/DeepAR/quantile/2/2ad27c696897fc216064f719a8bea178/DeepAR_best_val_QL.pt'}}, 'u': {0: {'cfg': 'final_weights/ETTh1/DeepAR/univariate/0/f7d0a9c9a4f592c7d2597ea802b6e624/ETTh1_prob.py', 'ckpt': 'final_weights/ETTh1/DeepAR/univariate/0/f7d0a9c9a4f592c7d2597ea802b6e624/DeepAR_best_val_NLL.pt'}, 2: {'cfg': 'final_weights/ETTh1/DeepAR/univariate/2/61d4606f2ce85d01757a4e462bf0517c/ETTh1_prob.py', 'ckpt': 'final_weights/ETTh1/DeepAR/univariate/2/61d4606f2ce85d01757a4e462bf0517c/DeepAR_best_val_NLL.pt'}}}}}\n"
     ]
    }
   ],
   "source": [
    "print(_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50c149e3-2506-4e3c-a8a3-e99a0f44c3b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-02 18:24:16,799 - easytorch-env - INFO - Disable TF32 mode\n",
      "2025-07-02 18:24:16,813 - easytorch-env - INFO - Use deterministic algorithms.\n",
      "2025-07-02 18:24:16,814 - easytorch-env - INFO - Set cudnn deterministic.\n",
      "2025-07-02 18:24:16,814 - easytorch - INFO - Set ckpt save dir: '/pfs/data6/home/ma/ma_ma/ma_kreffert/Probabilistic_LTSF/BasicTS/final_weightsETTh1/DeepAR/i_quantile/2/ae4525304ecb7ad6f13cf83b4f107bf5'\n",
      "2025-07-02 18:24:16,815 - easytorch - INFO - Building model.\n",
      "2025-07-02 18:24:17,254 - easytorch - INFO - Load model from : final_weights/ETTh1/DeepAR/i_quantile/2/10b4108b626a9d27fad40eddcff3715a/DeepAR_best_val_QL.pt\n",
      "2025-07-02 18:24:17,255 - easytorch - INFO - Loading Checkpoint from 'final_weights/ETTh1/DeepAR/i_quantile/2/10b4108b626a9d27fad40eddcff3715a/DeepAR_best_val_QL.pt'\n",
      "2025-07-02 18:24:17,278 - easytorch - INFO - Test dataset length: 2065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepAR\n",
      "Loading model checkpoint from final_weights/ETTh1/DeepAR/i_quantile/2/10b4108b626a9d27fad40eddcff3715a/DeepAR_best_val_QL.pt\n",
      "DeepAR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [20:12<00:00, 36.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2065, 720, 7, 9])\n",
      "torch.Size([2065, 720, 7, 1])\n",
      "torch.Size([2065, 96, 7, 1])\n",
      "torch.Size([2065, 720, 7, 100])\n",
      "saved under /pfs/data6/home/ma/ma_ma/ma_kreffert/Probabilistic_LTSF/BasicTS/final_weightsETTh1/DeepAR/i_quantile/2/model_return.pkl\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import gc\n",
    "import torch  # assuming PyTorch is being used\n",
    "from prob.prob_head import ProbabilisticHead \n",
    "\n",
    "def load_cfg(cfg, random_state=None):\n",
    "    from easytorch.config import init_cfg\n",
    "    # cfg path which start with dot will crash the easytorch, just remove dot\n",
    "    while isinstance(cfg, str) and cfg.startswith(('./','.\\\\')):\n",
    "        cfg = cfg[2:]\n",
    "    # while ckpt_path.startswith(('./','.\\\\')):\n",
    "    #     ckpt_path = ckpt_path[2:]\n",
    "    \n",
    "    # initialize the configuration\n",
    "    cfg = init_cfg(cfg, save=False)\n",
    "    return cfg\n",
    "    \n",
    "@torch.no_grad()\n",
    "def _forward(runner, cfg, data, epoch: int = None, iter_num: int = None, train: bool = True, ims=False, **kwargs):\n",
    "        \"\"\"\n",
    "        Performs the forward pass for training, validation, and testing. \n",
    "\n",
    "        Args:\n",
    "            data (Dict): A dictionary containing 'target' (future data) and 'inputs' (history data) (normalized by self.scaler).\n",
    "            epoch (int, optional): Current epoch number. Defaults to None.\n",
    "            iter_num (int, optional): Current iteration number. Defaults to None.\n",
    "            train (bool, optional): Indicates whether the forward pass is for training. Defaults to True.\n",
    "\n",
    "        Returns:\n",
    "            Dict: A dictionary containing the keys:\n",
    "                  - 'inputs': Selected input features.\n",
    "                  - 'prediction': Model predictions.\n",
    "                  - 'target': Selected target features.\n",
    "\n",
    "        Raises:\n",
    "            AssertionError: If the shape of the model output does not match [B, L, N].\n",
    "        \"\"\"\n",
    "        distribution_type = runner.distribution_type\n",
    "        model_name = cfg[\"MODEL\"][\"NAME\"]\n",
    "        if distribution_type in ['gaussian', 'student_t', 'laplace', 'm_lr_gaussian']:\n",
    "             prob_args = cfg['MODEL']['PARAM'].get('prob_args', None)\n",
    "             prob_head = ProbabilisticHead(1, 1, distribution_type, prob_args=prob_args)\n",
    "             sample = True\n",
    "        elif distribution_type in ['i_quantile']:\n",
    "            sample = True\n",
    "        else:\n",
    "            prob_head = None\n",
    "            sample = False\n",
    "        data = runner.preprocessing(data)\n",
    "\n",
    "        # Preprocess input data\n",
    "        future_data, history_data = data['target'], data['inputs']\n",
    "        history_data = runner.to_running_device(history_data)  # Shape: [B, L, N, C]\n",
    "        future_data = runner.to_running_device(future_data)    # Shape: [B, L, N, C]\n",
    "        batch_size, length, num_nodes, _ = future_data.shape\n",
    "\n",
    "        # Select input features\n",
    "        history_data = runner.select_input_features(history_data)\n",
    "        future_data_4_dec = runner.select_input_features(future_data)\n",
    "\n",
    "        if not train:\n",
    "            # For non-training phases, use only temporal features\n",
    "            future_data_4_dec[..., 0] = torch.empty_like(future_data_4_dec[..., 0])\n",
    "\n",
    "        # Forward pass through the model\n",
    "        if not (model_name == 'DeepAR' and distribution_type in ['i_quantile']):\n",
    "            model_return = runner.model(history_data=history_data, future_data=future_data_4_dec,\n",
    "                                        batch_seen=iter_num, epoch=epoch, train=train)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                model_return = runner.model.sample_trajectories(history_data=history_data, future_data=future_data_4_dec, num_samples=100)\n",
    "                model_return = model_return.permute(1, 2, 3, 0)\n",
    "        if distribution_type in ['gaussian', 'student_t', 'laplace', 'm_lr_gaussian']:\n",
    "            if model_name == 'DeepAR':\n",
    "                samples = runner.model.sample_trajectories(history_data=history_data, future_data=future_data_4_dec, num_samples=100)\n",
    "            else:\n",
    "                samples = prob_head.sample(model_return, num_samples=100, random_state=None) # shape torch.Size([num_samples, batch_size, output_len, num_series])\n",
    "            samples = samples.permute(1, 2, 3, 0)\n",
    "        elif distribution_type in ['i_quantile']:\n",
    "            samples = model_return[..., -100:] # torch.Size([64, 720, 7, 100])\n",
    "            model_return = model_return[..., :-100]\n",
    "\n",
    "        # Parse model return\n",
    "        if isinstance(model_return, torch.Tensor):\n",
    "            model_return = {'prediction': model_return}\n",
    "        if 'inputs' not in model_return:\n",
    "            model_return['inputs'] = runner.select_target_features(history_data)\n",
    "        if 'target' not in model_return:\n",
    "            model_return['target'] = runner.select_target_features(future_data)\n",
    "            \n",
    "        # Ensure the output shape is correct\n",
    "        assert list(model_return['prediction'].shape)[:3] == [batch_size, length, num_nodes], \\\n",
    "            f\"The shape of the output is incorrect. Ensure it matches [B, L, N, C]. Current {list(model_return['prediction'].shape)[:3]} != {[batch_size, length, num_nodes]}\"\n",
    "\n",
    "        model_return = runner.postprocessing(model_return)\n",
    "        if sample:\n",
    "            model_return['samples'] = runner.scaler.inverse_transform(samples, head='quantile')\n",
    "            assert list(model_return['samples'].shape)[:3] == [batch_size, length, num_nodes], \\\n",
    "            f\"The shape of the output is incorrect. Ensure it matches [B, L, N, C]. Current {list(model_return['samples'].shape)} != {[batch_size, length, num_nodes]}\"\n",
    "        return model_return\n",
    "            \n",
    "@torch.no_grad()\n",
    "def get_predictions(runner, cfg):\n",
    "    print(cfg[\"MODEL\"][\"NAME\"])\n",
    "    # init test\n",
    "    runner.test_interval = cfg['TEST'].get('INTERVAL', 1)\n",
    "    runner.test_data_loader = runner.build_test_data_loader(cfg)\n",
    "\n",
    "    runner.model.eval()\n",
    "    prediction, target, inputs = [], [], []\n",
    "    distribution_type = runner.distribution_type\n",
    "    if distribution_type in ['gaussian', 'student_t', 'laplace', 'm_lr_gaussian']:\n",
    "        sample = True\n",
    "        samples = []\n",
    "    if distribution_type in ['i_quantile']:\n",
    "        sample = True\n",
    "        samples = []\n",
    "    else:\n",
    "        sample = False\n",
    "    for data in tqdm(runner.test_data_loader):\n",
    "        # if model DeepAR -> forward with postprocessing of quantile and 100 sample trajectories!\n",
    "        \n",
    "        forward_return = _forward(runner, cfg, data, epoch=None, iter_num=None, train=False)\n",
    "        if not runner.if_evaluate_on_gpu:\n",
    "            forward_return['prediction'] = forward_return['prediction'].detach().cpu()\n",
    "            forward_return['target'] = forward_return['target'].detach().cpu()\n",
    "            forward_return['inputs'] = forward_return['inputs'].detach().cpu()\n",
    "            if sample:\n",
    "                forward_return['samples'] = forward_return['samples'].detach().cpu()\n",
    "                \n",
    "\n",
    "        prediction.append(forward_return['prediction'])\n",
    "        target.append(forward_return['target'])\n",
    "        inputs.append(forward_return['inputs'])\n",
    "        if sample:\n",
    "            samples.append(forward_return['samples'])\n",
    "\n",
    "    prediction = torch.cat(prediction, dim=0)\n",
    "    target = torch.cat(target, dim=0)\n",
    "    inputs = torch.cat(inputs, dim=0)\n",
    "    if sample:\n",
    "        samples = torch.cat(samples, dim=0)\n",
    "        return {'prediction': prediction, 'target': target, 'inputs': inputs, 'samples':samples}\n",
    "    else:\n",
    "        return {'prediction': prediction, 'target': target, 'inputs': inputs}\n",
    "    # configs[dataset][model][dist][random_state]['returns_all'] = returns_all\n",
    "\n",
    "def load_runner(configs):\n",
    "    for dataset in configs.keys():\n",
    "        for model in configs[dataset].keys():\n",
    "            for dist in configs[dataset][model].keys():\n",
    "                for random_state in configs[dataset][model][dist].keys():\n",
    "                    # configs[dataset][model][dist][random_state]['cfg'] = load_cfg(configs[dataset][model][dist][random_state]['cfg'])\n",
    "                    # cfg = configs[dataset][model][dist][random_state]['cfg']\n",
    "                    cfg = load_cfg(configs[dataset][model][dist][random_state]['cfg'])\n",
    "                    ckpt = configs[dataset][model][dist][random_state]['ckpt']\n",
    "                    strict=True\n",
    "                    runner = cfg['RUNNER'](cfg)\n",
    "                    # setup the graph if needed\n",
    "                    if runner.need_setup_graph:\n",
    "                        runner.setup_graph(cfg=cfg, train=False)\n",
    "                        \n",
    "                    print(f'Loading model checkpoint from {ckpt}')\n",
    "                    runner.load_model(ckpt_path=ckpt, strict=strict)\n",
    "                    \n",
    "                    # runner.test_pipeline(cfg=cfg, save_metrics=False, save_results=False)\n",
    "                    # configs[dataset][model][dist][random_state]['runner'] = runner\n",
    "\n",
    "                    # produce predictions\n",
    "                    returns_all = get_predictions(runner, cfg)\n",
    "                    print(returns_all['prediction'].shape)\n",
    "                    print(returns_all['target'].shape)\n",
    "                    print(returns_all['inputs'].shape)\n",
    "                    print(returns_all['samples'].shape)\n",
    "                    ckpt_dir = cfg['TRAIN'].get('CKPT_SAVE_DIR', 1)\n",
    "                    print(f'saved under {ckpt_dir}/model_return.pkl')\n",
    "                    with open(f'{ckpt_dir}/model_return.pkl', 'wb') as f:\n",
    "                        pickle.dump(returns_all, f)\n",
    "                    del runner\n",
    "                    del cfg\n",
    "                    del returns_all\n",
    "                    gc.collect()\n",
    "                    torch.cuda.empty_cache()\n",
    "    return configs\n",
    "\n",
    "# deepar = {'ETTh1': {'DeepAR': {'u': {2: {'cfg': 'final_weights/ETTh1/DeepAR/univariate/2/61d4606f2ce85d01757a4e462bf0517c/ETTh1_prob.py', 'ckpt': 'final_weights/ETTh1/DeepAR/univariate/2/61d4606f2ce85d01757a4e462bf0517c/DeepAR_best_val_NLL.pt'}}}}}\n",
    "# deepar = {'ETTh1': {'DLinear': {'iq': {2: {'cfg': 'final_weights/ETTh1/DLinear/i_quantile/2/7e59031295d7db4c807d3b5f9db7bc80/ETTh1_prob_quantile.py', 'ckpt': 'final_weights/ETTh1/DLinear/i_quantile/2/7e59031295d7db4c807d3b5f9db7bc80/DLinear_best_val_QL.pt'}}}}}\n",
    "deepar = {'ETTh1': {'DeepAR': {'iq': {2: {'cfg': 'final_weights/ETTh1/DeepAR/i_quantile/2/10b4108b626a9d27fad40eddcff3715a/ETTh1_prob_quantile.py', 'ckpt': 'final_weights/ETTh1/DeepAR/i_quantile/2/10b4108b626a9d27fad40eddcff3715a/DeepAR_best_val_QL.pt'}}}}}\n",
    "deepar = {'ETTh1': {'DLinear': {'q': {2: {'cfg': 'final_weights/ETTh1/DLinear/quantile/2/a7f10e8c211a68a5aeb51fbaec3c28a5/ETTh1_prob_quantile.py', 'ckpt': 'final_weights/ETTh1/DLinear/quantile/2/a7f10e8c211a68a5aeb51fbaec3c28a5/DLinear_best_val_QL.pt'}}}}}\n",
    "deepar = {'ETTh1': {'DeepAR': {'q': {2: {'cfg': 'final_weights/ETTh1/DeepAR/quantile/2/2ad27c696897fc216064f719a8bea178/ETTh1_prob_quantile.py', 'ckpt': 'final_weights/ETTh1/DeepAR/quantile/2/2ad27c696897fc216064f719a8bea178/DeepAR_best_val_QL.pt'}}}}}\n",
    "\n",
    "_configs = load_runner(deepar) #_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bfb3087-d6e8-4776-9073-f38579152b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model_return pickle files\n",
    "# evaluate the models\n",
    "import torch\n",
    "def vs_ensemble_torch(obs, fct, p=1.0):\n",
    "    \"\"\"\n",
    "    Compute Variogram Score using PyTorch on GPU.\n",
    "    obs: shape (..., D)\n",
    "    fct: shape (..., M, D)\n",
    "    \"\"\"\n",
    "    M = fct.shape[-2]\n",
    "\n",
    "    # Compute ensemble variogram component\n",
    "    fct_diff = fct.unsqueeze(-2) - fct.unsqueeze(-1)  # (B, M, D, D)\n",
    "    # print(fct_diff.shape)\n",
    "    vfct = (fct_diff.abs() ** p).sum(dim=-3) / M  # (B, D, D)\n",
    "    # print(vfct.shape)\n",
    "    # Compute observed variogram component\n",
    "    obs_diff = obs.unsqueeze(-2) - obs.unsqueeze(-1)  # (B, D, D)\n",
    "    vobs = (obs_diff.abs() ** p)  # (B, D, D)\n",
    "    # print(vobs.shape)\n",
    "    # print(vfct.shape)\n",
    "    vs = ((vfct - vobs) ** 2).sum(dim=(-2, -1))  # (B,)\n",
    "    return vs\n",
    "\n",
    "def es_ensemble_torch(obs: torch.Tensor, fct: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute the energy score using PyTorch.\n",
    "    \n",
    "    Parameters:\n",
    "    - obs: Tensor of shape (B, D)\n",
    "    - fct: Tensor of shape (B, M, D)\n",
    "\n",
    "    Returns:\n",
    "    - Tensor of shape (B,) with energy scores\n",
    "    \"\"\"\n",
    "    M = fct.shape[-2]\n",
    "\n",
    "    # E_1: mean norm between forecast samples and the observation\n",
    "    err_norm = torch.norm(fct - obs.unsqueeze(-2), dim=-1)  # (B, M)\n",
    "    E_1 = err_norm.sum(dim=-1) / M  # (B,)\n",
    "\n",
    "    # E_2: mean pairwise distance between forecast samples\n",
    "    spread = fct.unsqueeze(-3) - fct.unsqueeze(-2)  # (B, M, M, D)\n",
    "    spread_norm = torch.norm(spread, dim=-1)  # (B, M, M)\n",
    "    E_2 = spread_norm.sum(dim=(-2, -1)) / (M**2) # (B,)\n",
    "\n",
    "    return E_1 - 0.5 * E_2  # (B,)\n",
    "\n",
    "\n",
    "def coverage(y, forecast):\n",
    "    return np.mean(forecast >= y)\n",
    "    \n",
    "def sharpness(lower, upper, y):\n",
    "    y_safe = np.where(np.abs(y) == 0, 1e-7, np.abs(y))  # Avoid division by zero\n",
    "    return np.mean((upper - lower) / y_safe)\n",
    "\n",
    "def quantile_score(y, forecast, alpha):\n",
    "    \"\"\"\n",
    "    Quantile score for a given quantile level (alpha).\n",
    "    Lower is better.\n",
    "    \"\"\"\n",
    "    return np.mean(2 * ((y < forecast) - alpha) * (forecast - y))\n",
    "\n",
    "def interval_score(y, lower, upper, alpha):\n",
    "    \"\"\"\n",
    "    Interval score for central prediction intervals.\n",
    "    Lower is better.\n",
    "    \"\"\"\n",
    "    return np.mean((upper - lower) + \n",
    "                   (2 / alpha) * (lower - y) * (y < lower) + \n",
    "                   (2 / alpha) * (y - upper) * (y > upper))\n",
    "\n",
    "def weighted_quantile_score(y, forecast_quantiles, quantile_levels):\n",
    "    \"\"\"\n",
    "    Computes the weighted Quantile Score (wQS).\n",
    "    y: array of shape (D,)\n",
    "    forecast_quantiles: array of shape (len(quantile_levels), D)\n",
    "    \"\"\"\n",
    "    D = y.shape[0]\n",
    "    qs_sum = 0\n",
    "    for i, alpha in enumerate(quantile_levels):\n",
    "        qs = quantile_score(y, forecast_quantiles[i], alpha)\n",
    "        qs_sum += qs\n",
    "    y = np.where(np.abs(y) == 0, 1e-7, np.abs(y))  # Avoid division by zero\n",
    "    wqs = qs_sum / np.mean(np.abs(y)) if np.sum(np.abs(y)) != 0 else np.nan\n",
    "    return wqs / len(quantile_levels)\n",
    "\n",
    "\n",
    "def weighted_interval_score(y, forecast_quantiles, interval_bounds):\n",
    "    \"\"\"\n",
    "    Computes the Weighted Interval Score (WIS).\n",
    "    y: array of shape (D,)\n",
    "    forecast_quantiles: array of shape (len(quantile_levels), D)\n",
    "    interval_bounds: dict like {0.5: (0.25, 0.75), ...}\n",
    "    \"\"\"\n",
    "    median = forecast_quantiles[quantile_levels.index(0.5)]\n",
    "    wis = 0.5 * np.mean(np.abs(y - median))  # w0 * |y - median|\n",
    "\n",
    "    for alpha in interval_bounds:\n",
    "        lower_idx = quantile_levels.index(interval_bounds[alpha][0])\n",
    "        upper_idx = quantile_levels.index(interval_bounds[alpha][1])\n",
    "        lower = forecast_quantiles[lower_idx]\n",
    "        upper = forecast_quantiles[upper_idx]\n",
    "        iscore = interval_score(y, lower, upper, alpha=1 - alpha)\n",
    "        wis += (1 - alpha) * iscore / 2  # wk = alpha/2 = (1 - alpha)/2\n",
    "\n",
    "    K = len(interval_bounds)\n",
    "    return wis / (K + 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9c61721-50e7-46c3-84ff-34a967bb5fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [01:02<00:00,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'crps': np.float64(3.7991), 'crps_sum': np.float64(8.1743), 'crps_sum_q': np.float64(5.4013), 'crps_q': np.float64(2.5017), 'vs_05': np.float32(92.5215), 'vs_1': np.float32(3371.6697), 'vs_2': np.float32(2696077.2), 'es': np.float32(15.5537), 'wqs': np.float64(0.5429), 'wis': np.float64(2.5017), 'coverage_0.005': np.float64(0.0127), 'qs_0.005': np.float64(0.1565), 'coverage_0.025': np.float64(0.0584), 'qs_0.025': np.float64(0.7019), 'coverage_0.165': np.float64(0.169), 'qs_0.165': np.float64(3.8028), 'coverage_0.25': np.float64(0.1906), 'qs_0.25': np.float64(4.721), 'coverage_0.5': np.float64(0.4002), 'qs_0.5': np.float64(5.4637), 'coverage_0.75': np.float64(0.6762), 'qs_0.75': np.float64(4.0574), 'coverage_0.835': np.float64(0.7659), 'qs_0.835': np.float64(2.9832), 'coverage_0.975': np.float64(0.9738), 'qs_0.975': np.float64(0.5076), 'coverage_0.995': np.float64(0.996), 'qs_0.995': np.float64(0.1212), 'sharpness_0.5': np.float64(645551.196), 'is_0.5': np.float64(17.5568), 'sharpness_0.67': np.float64(938279.6044), 'is_0.67': np.float64(20.5639), 'sharpness_0.95': np.float64(1939040.511), 'is_0.95': np.float64(24.1904), 'sharpness_0.99': np.float64(2517127.2518), 'is_0.99': np.float64(27.777)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import scoringrules as sr\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "paths = ['/pfs/data6/home/ma/ma_ma/ma_kreffert/Probabilistic_LTSF/BasicTS/final_weightsETTh1/DeepAR/univariate/2/model_return.pkl',\n",
    "        '/pfs/data6/home/ma/ma_ma/ma_kreffert/Probabilistic_LTSF/BasicTS/final_weightsETTh1/DLinear/multivariate/2/model_return.pkl']\n",
    "\n",
    "    \n",
    "device = 'cuda:0'\n",
    "quantile_levels = [0.005, 0.025, 0.165, 0.25, 0.5, 0.75, 0.835, 0.975, 0.995]\n",
    "interval_bounds = {0.5: (0.25, 0.75), 0.67: (0.165, 0.835), 0.95: (0.025, 0.975), 0.99: (0.005, 0.995)}\n",
    "\n",
    "metrics = ['crps', 'crps_sum', 'crps_sum_q', 'crps_q', 'vs_05', 'vs_1', 'vs_2', 'es', 'wqs', 'wis']\n",
    "metrics = metrics + [f\"{m}{quantile}\" for quantile in quantile_levels for m in ['coverage_', 'qs_']]\n",
    "metrics = metrics + [f\"{m}{interval}\" for interval in interval_bounds.keys() for m in ['sharpness_', 'is_']]\n",
    "\n",
    "\n",
    "results = {}\n",
    "for metric in metrics:\n",
    "    results[metric] = [] \n",
    "\n",
    "path=paths[0]\n",
    "with open(path, 'rb') as f:\n",
    "    model_return = pickle.load(f)\n",
    "# inputs = model_return['inputs'].to(device)\n",
    "target = model_return['target'].to(device)\n",
    "samples = model_return['samples'].to(device)\n",
    "# Create DataLoader\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "batch_size = 64\n",
    "dataset = TensorDataset(samples, target)\n",
    "data_iter = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "for iter_index, (samples_batch, target_batch) in tqdm(enumerate(data_iter), total=len(data_iter)):\n",
    "    samples_batch = samples_batch.to(device)#.permute(0, 2, 3, 1)\n",
    "    target_batch = target_batch.to(device).squeeze(-1)\n",
    "    results['crps'].append(np.mean(sr.crps_ensemble(target_batch.detach().cpu(), samples_batch.detach().cpu(), estimator='pwm')))\n",
    "    results['crps_sum'] = np.mean(sr.crps_ensemble(target_batch.detach().cpu().sum(axis=-1), samples_batch.detach().cpu().sum(axis=-2), estimator='pwm'))\n",
    "    \n",
    "    # prediction=s\n",
    "    samples_batch = samples_batch.permute(0, 1, 3, 2)\n",
    "    results['vs_05'].append(torch.mean(vs_ensemble_torch(target_batch.to(device), samples_batch.to(device), p=0.5)).cpu().numpy())\n",
    "    # vs_1 = np.mean(sr.variogram_score(target.permute(0, 2, 1), samples.permute(0, 2, 3, 1), p=1.0, backend='numba'))\n",
    "    results['vs_1'].append(torch.mean(vs_ensemble_torch(target_batch.to(device), samples_batch.to(device), p=1)).cpu().numpy())\n",
    "    results['vs_2'].append(torch.mean(vs_ensemble_torch(target_batch.to(device), samples_batch.to(device), p=2)).cpu().numpy())\n",
    "    results['es'].append(torch.mean(es_ensemble_torch(target_batch.to(device), samples_batch.to(device))).cpu().numpy())\n",
    "    \n",
    "    # # now determine the quantiles of the forecasts and determine the quantile metrics\n",
    "    samples_batch = samples_batch.permute(0, 1, 3, 2).cpu().numpy()\n",
    "    target_batch = target_batch.cpu().numpy()\n",
    "    \n",
    "    quantiles = np.quantile(samples_batch.sum(axis=-2), quantile_levels, axis=-1)\n",
    "    results['crps_sum_q'].append(np.mean(sr.crps_quantile(target_batch.sum(axis=-1), np.transpose(quantiles, (1, 2, 0)), quantile_levels)))\n",
    "    \n",
    "    quantiles = np.quantile(samples_batch, quantile_levels, axis=-1)\n",
    "    results['crps_q'].append(np.mean(sr.crps_quantile(target_batch, np.transpose(quantiles, (1, 2, 3, 0)), quantile_levels)))\n",
    "    \n",
    "    # # evaluate interval metrics\n",
    "    for interval in interval_bounds.keys():\n",
    "        lower = torch.tensor(quantiles[quantile_levels.index(interval_bounds[interval][0])]).numpy()\n",
    "        upper = torch.tensor(quantiles[quantile_levels.index(interval_bounds[interval][1])]).numpy()\n",
    "        results[f'sharpness_{interval}'].append(sharpness(lower, upper, target_batch))\n",
    "        results[f'is_{interval}'].append(interval_score(target_batch, lower, upper, alpha=1 - interval))\n",
    "\n",
    "    for quantile in quantile_levels:\n",
    "        forecast = quantiles[quantile_levels.index(quantile)]\n",
    "        results[f'coverage_{quantile}'].append(coverage(target_batch, forecast))\n",
    "        results[f'qs_{quantile}'].append(quantile_score(target_batch, forecast, alpha=quantile))\n",
    "\n",
    "    # # Compute WQS and WIS\n",
    "    results['wqs'].append(weighted_quantile_score(target_batch, quantiles, quantile_levels))\n",
    "    results['wis'].append(weighted_interval_score(target_batch, quantiles, interval_bounds))\n",
    "    \n",
    "for metric in results.keys():\n",
    "    results[metric] = round(np.mean(results[metric]), 4)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997e790e-d691-4119-8b61-542ee8701b20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "180dba22-5b13-42c0-9957-76fdbb579a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/slurm_tmpdir/job_629247/ipykernel_1350581/1202630747.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  ground_truth = torch.tensor(samples[:, 51:])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dms_samples' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m metrics \u001b[38;5;241m=\u001b[39m metrics \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mm\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mquantile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m quantile \u001b[38;5;129;01min\u001b[39;00m quantile_levels \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoverage_\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mqs_\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m      9\u001b[0m metrics \u001b[38;5;241m=\u001b[39m metrics \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mm\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00minterval\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m interval \u001b[38;5;129;01min\u001b[39;00m interval_bounds\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msharpness_\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[0;32m---> 10\u001b[0m sample_list \u001b[38;5;241m=\u001b[39m [\u001b[43mdms_samples\u001b[49m, m_dms_samples, m_dms_2_samples, ims_samples]\n\u001b[1;32m     11\u001b[0m results \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDMS\u001b[39m\u001b[38;5;124m'\u001b[39m: {}, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mM_DMS\u001b[39m\u001b[38;5;124m'\u001b[39m:{}, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mM_DMS_2\u001b[39m\u001b[38;5;124m'\u001b[39m:{}, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIMS\u001b[39m\u001b[38;5;124m'\u001b[39m:{}}\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(sample_list):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dms_samples' is not defined"
     ]
    }
   ],
   "source": [
    "import scoringrules as sr\n",
    "ground_truth = torch.tensor(samples[:, 51:])\n",
    "\n",
    "device = 'cuda:0'\n",
    "quantile_levels = [0.005, 0.025, 0.165, 0.25, 0.5, 0.75, 0.835, 0.975, 0.995]\n",
    "interval_bounds = {0.5: (0.25, 0.75), 0.67: (0.165, 0.835), 0.95: (0.025, 0.975), 0.99: (0.005, 0.995)}\n",
    "metrics = ['crps', 'vs_05', 'vs_1', 'vs_2', 'es', 'wqs', 'wis']\n",
    "metrics = metrics + [f\"{m}{quantile}\" for quantile in quantile_levels for m in ['coverage_', 'qs_']]\n",
    "metrics = metrics + [f\"{m}{interval}\" for interval in interval_bounds.keys() for m in ['sharpness_', 'is_']]\n",
    "sample_list = [dms_samples, m_dms_samples, m_dms_2_samples, ims_samples]\n",
    "results = {'DMS': {}, 'M_DMS':{}, 'M_DMS_2':{}, 'IMS':{}}\n",
    "for i, s in enumerate(sample_list):\n",
    "    key = list(results.keys())[i]\n",
    "    for metric in metrics:\n",
    "        results[key][metric] = [] \n",
    "    for g in range(ground_truth.shape[0]):\n",
    "        target = ground_truth[g, :]\n",
    "        prediction = s.permute(1, 0)\n",
    "        results[key]['crps'].append(np.mean(sr.crps_ensemble(target.detach().cpu(), prediction.detach().cpu(), estimator='pwm')))\n",
    "        prediction=s\n",
    "        results[key]['vs_05'].append(torch.mean(vs_ensemble_torch(target.to(device), prediction.to(device), p=0.5)).cpu().numpy())\n",
    "        # vs_1 = np.mean(sr.variogram_score(target.permute(0, 2, 1), samples.permute(0, 2, 3, 1), p=1.0, backend='numba'))\n",
    "        results[key]['vs_1'].append(torch.mean(vs_ensemble_torch(target.to(device), prediction.to(device), p=1)).cpu().numpy())\n",
    "        results[key]['vs_2'].append(torch.mean(vs_ensemble_torch(target.to(device), prediction.to(device), p=2)).cpu().numpy())\n",
    "        results[key]['es'].append(torch.mean(es_ensemble_torch(target.to(device), prediction.to(device))).cpu().numpy())\n",
    "        \n",
    "        # now determine the quantiles of the forecasts and determine the quantile metrics\n",
    "        quantiles = np.quantile(prediction, quantile_levels, axis=0)\n",
    "        \n",
    "        # evaluate interval metrics\n",
    "        for interval in interval_bounds.keys():\n",
    "            lower = torch.tensor(quantiles[quantile_levels.index(interval_bounds[interval][0])]).numpy()\n",
    "            upper = torch.tensor(quantiles[quantile_levels.index(interval_bounds[interval][1])]).numpy()\n",
    "            results[key][f'sharpness_{interval}'].append(sharpness(lower, upper, target.numpy()))\n",
    "            results[key][f'is_{interval}'].append(interval_score(target.numpy(), lower, upper, alpha=1 - interval))\n",
    "\n",
    "        for quantile in quantile_levels:\n",
    "            forecast = quantiles[quantile_levels.index(quantile)]\n",
    "            results[key][f'coverage_{quantile}'].append(coverage(target.numpy(), forecast))\n",
    "            results[key][f'qs_{quantile}'].append(quantile_score(target.numpy(), forecast, alpha=quantile))\n",
    "\n",
    "        # Compute WQS and WIS\n",
    "        results[key]['wqs'].append(weighted_quantile_score(target.numpy(), quantiles, quantile_levels))\n",
    "        results[key]['wis'].append(weighted_interval_score(target.numpy(), quantiles, interval_bounds))\n",
    "\n",
    "        \n",
    "    for metric in results[key].keys():\n",
    "        results[key][metric] = round(np.mean(results[key][metric]), 4)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb54ae68-293e-4b3c-8665-71afa166768d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3cefb2-2568-4f9b-8066-09f64c38ba0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bc12d0-4f95-40e1-b510-4a4d39a2b742",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42f78b7-8119-40ca-b48a-b1b58d07a4d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3915fb55-3974-4d5d-8c75-1ecbd30e2651",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples(X_train, dms_model, m_dms_model, m_dms_2_model, ims_model, num_samples=50, random_state=42):\n",
    "    # dms samples\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    prefix= X_train[:1, :].to(device)\n",
    "    with torch.no_grad():\n",
    "        dms_model.eval()\n",
    "        head_output = dms_model(prefix)\n",
    "        dms_samples = dms_model.sample(head_output, num_samples=num_samples, random_state=random_state)\n",
    "        dms_samples = dms_samples.squeeze()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        m_dms_model.eval()\n",
    "        head_output = m_dms_model(prefix)\n",
    "        m_dms_samples = m_dms_model.sample(head_output, num_samples=num_samples, random_state=random_state)\n",
    "        m_dms_samples = m_dms_samples.squeeze()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        m_dms_2_model.eval()\n",
    "        head_output = m_dms_2_model(prefix)\n",
    "        m_dms_2_samples = m_dms_2_model.sample(head_output, num_samples=num_samples, random_state=random_state)\n",
    "        m_dms_2_samples = m_dms_2_samples.squeeze()\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model_ims.eval()\n",
    "        prefix_tensor = torch.tensor(prefix, dtype=torch.float32).unsqueeze(-1).to(device)\n",
    "        ims_samples, sigmas_ims = model_ims.forecast(prefix_tensor, steps=51, use_mean=False, num_samples=num_samples)\n",
    "        ims_samples = ims_samples.squeeze()\n",
    "\n",
    "    return dms_samples, m_dms_samples, m_dms_2_samples, ims_samples\n",
    "   \n",
    "    \n",
    "\n",
    "\n",
    "def plot_samples(X_train, Y_train, dms_samples, m_dms_samples, m_dms_2_samples, ims_samples):\n",
    "    custom_text_color = \"#333333\"\n",
    "    custom_bg = \"#FDFDFF\"\n",
    "    \n",
    "    custom_theme = {\n",
    "        'axes.facecolor': custom_bg,\n",
    "        'axes.edgecolor': 'black',\n",
    "        'axes.grid': False,\n",
    "        'grid.color': '#dcdcdc',\n",
    "        'grid.linestyle': '-',\n",
    "        'xtick.color': custom_text_color,\n",
    "        'ytick.color': custom_text_color,\n",
    "        'axes.labelcolor': custom_text_color,\n",
    "        'text.color': custom_text_color\n",
    "    }\n",
    "    sns.set_theme(style='white')\n",
    "    sns.set_style(\"white\", rc=custom_theme)\n",
    "    fig, axs = plt.subplots(nrows=5, figsize=(20, 20), sharey=True, sharex=True)\n",
    "\n",
    "    # ground truth trajectory\n",
    "    # lambda x: np.sin(50/4 * (x + self.prefix_len))     # World 0: continues sine\n",
    "    # lambda x: -np.sin(50/4 * (x + self.prefix_len))    # World 2: cosine\n",
    "    prefix = X_train[:1, :]\n",
    "    offset = 5\n",
    "    world_1 = [np.sin(50/4 * (x + 51)) for x in np.arange(prefix.shape[1]-offset, prefix.shape[1] + Y_train.shape[1]-offset)]\n",
    "    world_2 = [-np.sin(50/4 * (x + 51)) for x in np.arange(prefix.shape[1]-offset, prefix.shape[1] + Y_train.shape[1]-offset)]\n",
    "\n",
    "    \n",
    "    # ground truth samples\n",
    "    for s in range(Y_train.shape[0]):\n",
    "        if s ==0:\n",
    "            axs[0].plot(np.arange(prefix.shape[1], prefix.shape[1]+ Y_train.shape[1]), Y_train[s, :], color='black', alpha=0.2, label='Ground Truth Samples')\n",
    "        else:\n",
    "            axs[0].plot(np.arange(prefix.shape[1], prefix.shape[1]+ Y_train.shape[1]), Y_train[s, :], color='black', alpha=0.2)\n",
    "    axs[0].plot(np.arange(prefix.shape[1], prefix.shape[1]+ Y_train.shape[1]), world_1, color='orange', linestyle='--', linewidth=2.5, label=\"$\\sin(x)$-Base\")\n",
    "    axs[0].plot(np.arange(prefix.shape[1], prefix.shape[1]+ Y_train.shape[1]), world_2, color='orange', linestyle='--', linewidth=2.5, label=\"$-1\\cdot\\sin(x)$-Base\")\n",
    "\n",
    "    # dms samples\n",
    "    # axs[1].plot(np.arange(prefix.shape[1]), prefix[0, :])\n",
    "    for s in range(dms_samples.shape[0]):\n",
    "        if s ==0:\n",
    "            axs[1].plot(np.arange(prefix.shape[1], prefix.shape[1]+ dms_samples.shape[1]), dms_samples[s, :], color='blue', alpha=0.2, label='Sample Predictions')\n",
    "        else:\n",
    "            axs[1].plot(np.arange(prefix.shape[1], prefix.shape[1]+ dms_samples.shape[1]), dms_samples[s, :], color='blue', alpha=0.2)\n",
    "    axs[1].plot(np.arange(prefix.shape[1], prefix.shape[1]+ dms_samples.shape[1]), world_1, color='orange', linestyle='--', linewidth=2.5)\n",
    "    axs[1].plot(np.arange(prefix.shape[1], prefix.shape[1]+ dms_samples.shape[1]), world_2, color='orange', linestyle='--', linewidth=2.5)\n",
    "    \n",
    "        \n",
    "    # dms multivariate samples\n",
    "    # axs[2].plot(np.arange(prefix.shape[1]), prefix[0, :])\n",
    "    for s in range(m_dms_samples.shape[0]):\n",
    "        axs[2].plot(np.arange(prefix.shape[1], prefix.shape[1]+ m_dms_samples.shape[1]), m_dms_samples[s, :], color='blue', alpha=0.2)\n",
    "    axs[2].plot(np.arange(prefix.shape[1], prefix.shape[1]+ m_dms_samples.shape[1]), world_1, color='orange', linestyle='--', linewidth=2.5)\n",
    "    axs[2].plot(np.arange(prefix.shape[1], prefix.shape[1]+ m_dms_samples.shape[1]), world_2, color='orange', linestyle='--', linewidth=2.5)\n",
    "    # axs[2].\n",
    "    for s in range(m_dms_2_samples.shape[0]):\n",
    "        axs[3].plot(np.arange(prefix.shape[1], prefix.shape[1]+ m_dms_2_samples.shape[1]), m_dms_2_samples[s, :], color='blue', alpha=0.2)\n",
    "    axs[3].plot(np.arange(prefix.shape[1], prefix.shape[1]+ m_dms_2_samples.shape[1]), world_1, color='orange', linestyle='--', linewidth=2.5)\n",
    "    axs[3].plot(np.arange(prefix.shape[1], prefix.shape[1]+ m_dms_2_samples.shape[1]), world_2, color='orange', linestyle='--', linewidth=2.5)\n",
    "    # axs[2].\n",
    "    \n",
    "    # ims samples\n",
    "    for s in range(ims_samples.shape[0]):\n",
    "        axs[4].plot(np.arange(prefix.shape[1], prefix.shape[1]+ ims_samples.shape[1]), ims_samples[s, :], color='blue', alpha=0.2)\n",
    "    axs[4].plot(np.arange(prefix.shape[1], prefix.shape[1]+ ims_samples.shape[1]), world_1, color='orange', linestyle='--', linewidth=2.5)\n",
    "    axs[4].plot(np.arange(prefix.shape[1], prefix.shape[1]+ ims_samples.shape[1]), world_2, color='orange', linestyle='--', linewidth=2.5)\n",
    "    \n",
    "    for ax in axs:\n",
    "        ax.axvline(prefix.shape[1], color=\"black\", linestyle=\"--\", label=\"Branching Point\")\n",
    "        ax.set_ylim(-2, 2) \n",
    "        ax.tick_params(axis='both', labelsize=14)\n",
    "        # ax.set_ylabel('Value', fontsize=20, fontweight='bold')\n",
    "    axs[-1].set_xlabel('Time', fontsize=20, fontweight='bold')\n",
    "    labels = ['Ground Truth', 'DMS Model (univariate)', 'DMS Model (multivariate-full rank)', 'DMS Model (multivariate-low rank)', 'IMS Model']\n",
    "    for i, ax in enumerate(axs):\n",
    "        ax.text(\n",
    "            0.5, 0.97, labels[i],\n",
    "            transform=ax.transAxes,\n",
    "            fontsize=18,\n",
    "            fontweight='bold',\n",
    "            verticalalignment='top',\n",
    "            horizontalalignment = 'center',\n",
    "            bbox=dict(boxstyle='round', facecolor='white', edgecolor='black')\n",
    "        )\n",
    "    plt.tight_layout()\n",
    "    sns.despine(left=False, bottom=False)\n",
    "    # Create a single legend for the entire figure\n",
    "    # Collect handles and labels from the last subplot (they should be consistent across all subplots)\n",
    "    # Collect all handles and labels from all subplots\n",
    "    all_handles = []\n",
    "    all_labels = []\n",
    "    for ax in axs:\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "        all_handles.extend(handles)\n",
    "        all_labels.extend(labels)\n",
    "    \n",
    "    # Remove duplicates while preserving order\n",
    "    from collections import OrderedDict\n",
    "    from matplotlib.lines import Line2D\n",
    "    unique = list(OrderedDict.fromkeys(zip(all_handles, all_labels)))\n",
    "    handles, labels = zip(*unique)\n",
    "    \n",
    "    # Create proxy handles with alpha=1 for the legend\n",
    "    legend_handles = []\n",
    "    for h in handles[:5]:\n",
    "        if isinstance(h, Line2D):\n",
    "            legend_handles.append(Line2D([], [], color=h.get_color(), linestyle=h.get_linestyle(), linewidth=h.get_linewidth(), alpha=1))\n",
    "        else:\n",
    "            legend_handles.append(h)\n",
    "    # handles_1, labels_1 = axs[1].get_legend_handles_labels()\n",
    "    # handles = [handles[:], handles_1[:]]\n",
    "    # handles = [handles[i] for i in [0, 1, 4, 7]]\n",
    "    # labels = [labels[i] for i in [0]]+ ['Cooling', 'Idle', 'Heavy Load']\n",
    "    # Place the legend outside the subplots\n",
    "    fig.legend(legend_handles, labels[:5], loc='lower center', ncol=1, #len(labels), \n",
    "              bbox_to_anchor=(0.9, 0.77), fontsize=14, frameon=True, framealpha=1)\n",
    "\n",
    "X_train, Y_train = build_dms_dataset(samples, 50, 51)\n",
    "dms_samples, m_dms_samples, m_dms_2_samples, ims_samples = get_samples(X_train, g_dms, m_lr_dms, m_dms_2_model, model_ims, num_samples=20, random_state=42)\n",
    "X_train, Y_train, dms_samples, m_dms_samples, m_dms_2_samples, ims_samples = X_train.cpu(), Y_train.cpu(), dms_samples.cpu(), m_dms_samples.cpu(), m_dms_2_samples.cpu(), ims_samples.cpu()\n",
    "plot_samples(X_train, Y_train, dms_samples, m_dms_samples, m_dms_2_samples, ims_samples)\n",
    "plt.savefig(\"Synthetic_plots.pdf\", format='pdf', dpi=600, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2d6fe5-8ac9-4d40-ad7b-209cd49a7285",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.ndimage import label\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "def get_kde_hdr_intervals(x, level=0.5, bandwidth=0.1, grid_size=1000):\n",
    "    \"\"\"\n",
    "    Estimate HDR intervals using Kernel Density Estimation.\n",
    "    \"\"\"\n",
    "    x = x.flatten()\n",
    "    kde = KernelDensity(kernel='gaussian', bandwidth=bandwidth).fit(x[:, None])\n",
    "    \n",
    "    x_grid = np.linspace(np.min(x) - 0.5, np.max(x) + 0.5, grid_size)\n",
    "    log_density = kde.score_samples(x_grid[:, None])\n",
    "    density = np.exp(log_density)\n",
    "    \n",
    "    idx_sorted = np.argsort(-density)\n",
    "    cumulative = np.cumsum(density[idx_sorted])\n",
    "    cumulative /= cumulative[-1]\n",
    "\n",
    "    threshold_idx = idx_sorted[cumulative <= level]\n",
    "    hdr_mask = np.zeros_like(density, dtype=bool)\n",
    "    hdr_mask[threshold_idx] = True\n",
    "    \n",
    "    labeled, num_features = label(hdr_mask)\n",
    "    intervals = []\n",
    "    for i in range(1, num_features + 1):\n",
    "        region_idx = np.where(labeled == i)[0]\n",
    "        if len(region_idx) > 0:\n",
    "            low = x_grid[region_idx[0]]\n",
    "            high = x_grid[region_idx[-1]]\n",
    "            intervals.append((low, high))\n",
    "    return intervals\n",
    "\n",
    "def plot_sample_intervals(X_train, Y_train, dms_samples, m_dms_samples, m_dms_2_samples, ims_samples):\n",
    "    custom_text_color = \"#333333\"\n",
    "    custom_bg = \"#FDFDFF\"\n",
    "    custom_theme = {\n",
    "        'axes.facecolor': custom_bg,\n",
    "        'axes.edgecolor': 'black',\n",
    "        'axes.grid': False,\n",
    "        'grid.color': '#dcdcdc',\n",
    "        'xtick.color': custom_text_color,\n",
    "        'ytick.color': custom_text_color,\n",
    "        'axes.labelcolor': custom_text_color,\n",
    "        'text.color': custom_text_color\n",
    "    }\n",
    "\n",
    "    sns.set_theme(style='white')\n",
    "    sns.set_style(\"white\", rc=custom_theme)\n",
    "    fig, axs = plt.subplots(nrows=5, figsize=(20, 20), sharey=True, sharex=True)\n",
    "\n",
    "    prefix = X_train[:1, :]\n",
    "    offset = 5\n",
    "    t = np.arange(prefix.shape[1], prefix.shape[1] + Y_train.shape[1])\n",
    "    world_1 = [np.sin(50/4 * (x + 51)) for x in np.arange(prefix.shape[1]-offset, prefix.shape[1] + Y_train.shape[1]-offset)]\n",
    "    world_2 = [-np.sin(50/4 * (x + 51)) for x in np.arange(prefix.shape[1]-offset, prefix.shape[1] + Y_train.shape[1]-offset)]\n",
    "\n",
    "    def plot_ci(ax, samples, color, label=None):\n",
    "        median = np.median(samples, axis=0)\n",
    "        p25 = np.percentile(samples, 25, axis=0)\n",
    "        p75 = np.percentile(samples, 75, axis=0)\n",
    "        p5 = np.percentile(samples, 5, axis=0)\n",
    "        p95 = np.percentile(samples, 95, axis=0)\n",
    "\n",
    "        ax.plot(t, median, color=color, label=label, linewidth=2)\n",
    "        ax.fill_between(t, p25, p75, color=color, alpha=0.3, label=\"50% CI\" if label else None)\n",
    "        ax.fill_between(t, p5, p95, color=color, alpha=0.1, label=\"90% CI\" if label else None)\n",
    "    \n",
    "    def get_hdr_intervals(x, density, level=0.5):\n",
    "        n_samples = len(x)\n",
    "        n_hdr = int(n_samples * level)\n",
    "        top_idx = np.argsort(density)[-n_hdr:]\n",
    "        hdr_points = np.zeros(n_samples, dtype=bool)\n",
    "        hdr_points[top_idx] = True\n",
    "    \n",
    "        # Sort by x for connected component analysis\n",
    "        sorted_idx = np.argsort(x.flatten())\n",
    "        mask_sorted = hdr_points[sorted_idx]\n",
    "        \n",
    "        # Label contiguous high-density regions\n",
    "        labeled, num_features = label(mask_sorted)\n",
    "        intervals = []\n",
    "        for i in range(1, num_features + 1):\n",
    "            region_idx = sorted_idx[labeled == i]\n",
    "            region_vals = x[region_idx]\n",
    "            intervals.append((np.min(region_vals), np.max(region_vals)))\n",
    "        return intervals\n",
    "    \n",
    "    def plot_ci_knn(ax, samples, color, label=None, alpha_50=0.3, alpha_90=0.1, k=10):\n",
    "        n_samples, n_timesteps = samples.shape\n",
    "        # t = np.arange(n_timesteps)\n",
    "        median = np.median(samples, axis=0)\n",
    "    \n",
    "        for t_idx in range(n_timesteps):\n",
    "            x = samples[:, t_idx].reshape(-1, 1)\n",
    "            nbrs = NearestNeighbors(n_neighbors=k).fit(x)\n",
    "            distances, _ = nbrs.kneighbors(x)\n",
    "            density = -np.mean(distances, axis=1)  # Higher density = less distance\n",
    "    \n",
    "            # Plot 50% HDR\n",
    "            intervals_50 = get_hdr_intervals(x, density, level=0.5)\n",
    "            for low, high in intervals_50:\n",
    "                ax.fill_between([t[t_idx]-0.4, t[t_idx]+0.4], [low, low], [high, high],\n",
    "                                color=color, alpha=alpha_50)\n",
    "    \n",
    "            # Plot 90% HDR\n",
    "            intervals_90 = get_hdr_intervals(x, density, level=0.9)\n",
    "            for low, high in intervals_90:\n",
    "                ax.fill_between([t[t_idx]-0.4, t[t_idx]+0.4], [low, low], [high, high],\n",
    "                                color=color, alpha=alpha_90)\n",
    "    \n",
    "        ax.plot(t, median, color=color, label=label, linewidth=2)\n",
    "    def plot_ci_kde(ax, samples, color, label=None, alpha_50=0.3, alpha_90=0.1, bandwidth=0.1):\n",
    "        n_samples, n_timesteps = samples.shape\n",
    "        median = np.median(samples, axis=0)\n",
    "        \n",
    "        for t_idx in range(n_timesteps):\n",
    "            x = samples[:, t_idx]\n",
    "            \n",
    "            # Plot 50% HDR\n",
    "            intervals_50 = get_kde_hdr_intervals(x, level=0.5, bandwidth=bandwidth)\n",
    "            for low, high in intervals_50:\n",
    "                ax.fill_between([t[t_idx]-0.4, t[t_idx]+0.4], [low, low], [high, high],\n",
    "                                color=color, alpha=alpha_50)\n",
    "            \n",
    "            # Plot 90% HDR\n",
    "            intervals_90 = get_kde_hdr_intervals(x, level=0.9, bandwidth=bandwidth)\n",
    "            for low, high in intervals_90:\n",
    "                ax.fill_between([t[t_idx]-0.4, t[t_idx]+0.4], [low, low], [high, high],\n",
    "                                color=color, alpha=alpha_90)\n",
    "        \n",
    "        ax.plot(t, median, color=color, label=label, linewidth=2)\n",
    "\n",
    "\n",
    "\n",
    "    # Ground Truth\n",
    "    plot_ci_kde(axs[0], Y_train.numpy(), 'black', label='Ground Truth Median')\n",
    "    # plot_ci(axs[0], Y_train.numpy(), 'black', label='Ground Truth Median')\n",
    "    \n",
    "    axs[0].plot(t, world_1, color='orange', linestyle='--', linewidth=2.5, label=\"$\\sin(x)$-Base\")\n",
    "    axs[0].plot(t, world_2, color='orange', linestyle='--', linewidth=2.5, label=\"$-1\\cdot\\sin(x)$-Base\")\n",
    "\n",
    "    # DMS Models\n",
    "    plot_ci_kde(axs[1], dms_samples.numpy(), 'blue')\n",
    "    plot_ci_kde(axs[2], m_dms_samples.numpy(), 'blue')\n",
    "    plot_ci_kde(axs[3], m_dms_2_samples.numpy(), 'blue')\n",
    "    plot_ci_kde(axs[4], ims_samples.numpy(), 'blue')\n",
    "    # plot_ci(axs[1], dms_samples.numpy(), 'blue')\n",
    "    # plot_ci(axs[2], m_dms_samples.numpy(), 'blue')\n",
    "    # plot_ci(axs[3], m_dms_2_samples.numpy(), 'blue')\n",
    "    # plot_ci(axs[4], ims_samples.numpy(), 'blue')\n",
    "\n",
    "    \n",
    "    for i, ax in enumerate(axs):\n",
    "        ax.plot(t, world_1, color='orange', linestyle='--', linewidth=2.5)\n",
    "        ax.plot(t, world_2, color='orange', linestyle='--', linewidth=2.5) \n",
    "        ax.axvline(prefix.shape[1], color=\"black\", linestyle=\"--\", label=\"Branching Point\")\n",
    "        ax.set_ylim(-2, 2)\n",
    "        ax.tick_params(axis='both', labelsize=14)\n",
    "\n",
    "    axs[-1].set_xlabel('Time', fontsize=20, fontweight='bold')\n",
    "    labels = ['Ground Truth', 'DMS Model (univariate)', 'DMS Model (multivariate-full rank)', 'DMS Model (multivariate-low rank)', 'IMS Model']\n",
    "    for i, ax in enumerate(axs):\n",
    "        ax.text(\n",
    "            0.5, 0.97, labels[i],\n",
    "            transform=ax.transAxes,\n",
    "            fontsize=18,\n",
    "            fontweight='bold',\n",
    "            verticalalignment='top',\n",
    "            horizontalalignment = 'center',\n",
    "            bbox=dict(boxstyle='round', facecolor='white', edgecolor='black')\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    sns.despine(left=False, bottom=False)\n",
    "\n",
    "    # Legend (only first subplot)\n",
    "    handles, labels = axs[0].get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc='lower center', ncol=2, bbox_to_anchor=(0.85, 0.79), fontsize=14, frameon=True, framealpha=1)\n",
    "dms_samples, m_dms_samples, m_dms_2_samples, ims_samples = get_samples(X_train, g_dms, m_lr_dms, m_dms_2_model, model_ims, num_samples=100, random_state=42)\n",
    "X_train, Y_train, dms_samples, m_dms_samples, m_dms_2_samples, ims_samples = X_train.cpu(), Y_train.cpu(), dms_samples.cpu(), m_dms_samples.cpu(), m_dms_2_samples.cpu(), ims_samples.cpu()\n",
    "plot_sample_intervals(X_train, Y_train, dms_samples, m_dms_samples, m_dms_2_samples, ims_samples)\n",
    "plt.savefig(\"Synthetic_plots_intervals_knn.pdf\", format='pdf', dpi=600, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c7f1fc-3a76-4d42-a046-0b58eb30f91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "@torch.no_grad()\n",
    "def get_predictions(configs):\n",
    "    for dataset in configs.keys():\n",
    "        for model in configs[dataset].keys():\n",
    "            for dist in configs[dataset][model].keys():\n",
    "                for random_state in configs[dataset][model][dist].keys():\n",
    "                    runner = configs[dataset][model][dist][random_state]['runner']\n",
    "                    cfg = configs[dataset][model][dist][random_state]['cfg']\n",
    "                    # init test\n",
    "                    runner.test_interval = cfg['TEST'].get('INTERVAL', 1)\n",
    "                    runner.test_data_loader = runner.build_test_data_loader(cfg)\n",
    "                \n",
    "                    runner.model.eval()\n",
    "                    prediction, target, inputs = [], [], []\n",
    "                \n",
    "                    for data in tqdm(runner.test_data_loader):\n",
    "                        # if model DeepAR -> forward with postprocessing of quantile and 100 sample trajectories!\n",
    "                        \n",
    "                        forward_return = runner.forward(data, epoch=None, iter_num=None, train=False)\n",
    "                        if not runner.if_evaluate_on_gpu:\n",
    "                            forward_return['prediction'] = forward_return['prediction'].detach().cpu()\n",
    "                            forward_return['target'] = forward_return['target'].detach().cpu()\n",
    "                            forward_return['inputs'] = forward_return['inputs'].detach().cpu()\n",
    "                \n",
    "                        prediction.append(forward_return['prediction'])\n",
    "                        target.append(forward_return['target'])\n",
    "                        inputs.append(forward_return['inputs'])\n",
    "                \n",
    "                    prediction = torch.cat(prediction, dim=0)\n",
    "                    target = torch.cat(target, dim=0)\n",
    "                    inputs = torch.cat(inputs, dim=0)\n",
    "                \n",
    "                    returns_all = {'prediction': prediction, 'target': target, 'inputs': inputs}\n",
    "                    configs[dataset][model][dist][random_state]['returns_all'] = returns_all\n",
    "    return configs\n",
    "\n",
    "_configs = get_predictions(_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892e5d76-dd7c-41e5-beb1-79f117e04dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. load the model and set the device\n",
    "_configs = {'ETTh1_PTST_u': {'cfg':'final_weights/PatchTST/univariate/ETTh1_prob.py',\n",
    "                           'ckpt': 'final_weights/PatchTST/univariate/ETTh1_100_96_720/a8de06edad7530010e0b704422b431a2/PatchTST_best_val_NLL.pt'\n",
    "                          },\n",
    "           # 'ETTh1_PTST_q': {'cfg': 'final_weights/PatchTST/quantile/ETTh1_prob.py',\n",
    "           #                  'ckpt': 'final_weights/PatchTST/quantile/ETTh1_100_96_720/a2a39ac1680165e5ffbda2c7bbda5add/PatchTST_best_val_QL.pt'\n",
    "           #                 }\n",
    "          }\n",
    "\n",
    "random_states = range(5)\n",
    "\n",
    "configs = {rs:_configs for rs in random_states}\n",
    "\n",
    "configs = load_runner(configs, random_states=random_states)\n",
    "configs = get_predictions(configs)\n",
    "# metrics_results = self.compute_evaluation_metrics(returns_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad2779b-e0b2-42e1-b694-91ae0decfeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def vs_ensemble_torch(obs, fct, p=1.0):\n",
    "    \"\"\"\n",
    "    Compute Variogram Score using PyTorch on GPU.\n",
    "    obs: shape (..., D)\n",
    "    fct: shape (..., M, D)\n",
    "    \"\"\"\n",
    "    M = fct.shape[-2]\n",
    "\n",
    "    # Compute ensemble variogram component\n",
    "    fct_diff = fct.unsqueeze(-2) - fct.unsqueeze(-1)  # (B, M, D, D)\n",
    "    # print(fct_diff.shape)\n",
    "    vfct = (fct_diff.abs() ** p).sum(dim=-3) / M  # (B, D, D)\n",
    "    # print(vfct.shape)\n",
    "    # Compute observed variogram component\n",
    "    obs_diff = obs.unsqueeze(-2) - obs.unsqueeze(-1)  # (B, D, D)\n",
    "    vobs = (obs_diff.abs() ** p)  # (B, D, D)\n",
    "    # print(vobs.shape)\n",
    "    # print(vfct.shape)\n",
    "    vs = ((vfct - vobs) ** 2).sum(dim=(-2, -1))  # (B,)\n",
    "    return vs\n",
    "\n",
    "def es_ensemble_torch(obs: torch.Tensor, fct: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute the energy score using PyTorch.\n",
    "    \n",
    "    Parameters:\n",
    "    - obs: Tensor of shape (B, D)\n",
    "    - fct: Tensor of shape (B, M, D)\n",
    "\n",
    "    Returns:\n",
    "    - Tensor of shape (B,) with energy scores\n",
    "    \"\"\"\n",
    "    M = fct.shape[-2]\n",
    "\n",
    "    # E_1: mean norm between forecast samples and the observation\n",
    "    err_norm = torch.norm(fct - obs.unsqueeze(-2), dim=-1)  # (B, M)\n",
    "    E_1 = err_norm.sum(dim=-1) / M  # (B,)\n",
    "\n",
    "    # E_2: mean pairwise distance between forecast samples\n",
    "    spread = fct.unsqueeze(-3) - fct.unsqueeze(-2)  # (B, M, M, D)\n",
    "    spread_norm = torch.norm(spread, dim=-1)  # (B, M, M)\n",
    "    E_2 = spread_norm.sum(dim=(-2, -1)) / (M**2) # (B,)\n",
    "\n",
    "    return E_1 - 0.5 * E_2  # (B,)\n",
    "\n",
    "def sample(runner, returns_all, random_state=None):\n",
    "    from prob.prob_head import ProbabilisticHead # load that class for sampling\n",
    "    head = ProbabilisticHead(1, 1, runner.distribution_type, prob_args=runner.prob_args)\n",
    "    samples = []\n",
    "    batch_size = 64\n",
    "    num_batches = int(returns_all['prediction'].shape[0]/batch_size)+1\n",
    "    for b in range(num_batches):\n",
    "        start, end = b*batch_size, min((b+1)*batch_size, returns_all['prediction'].shape[0])\n",
    "        pred = returns_all['prediction'][start:end, :, :, :]\n",
    "        sample = head.sample(pred, num_samples=100, random_state=random_state) # [samples x bs x seq_len x nvars]\n",
    "        sample = sample.permute(1, 0, 2, 3)       # [bs x samples x seq_len x nvars]\n",
    "        samples.append(sample)\n",
    "    samples = torch.cat(samples, dim=0)\n",
    "    return samples\n",
    "\n",
    "def evaluate(predictions, returns_all, batch_size=4):\n",
    "    import scoringrules as sr\n",
    "    import numpy as np\n",
    "    device = returns_all['target'].device\n",
    "    targets = returns_all['target'].squeeze(-1)#.detach().cpu()\n",
    "    sampless = predictions.permute(0, 2, 3, 1)#.detach().cpu() \n",
    "    num_batches = int(returns_all['prediction'].shape[0]/batch_size)+1\n",
    "    # Lists to accumulate metric values\n",
    "    crps_list = []\n",
    "    crps_sum_list = []\n",
    "    vs_05_list = []\n",
    "    vs_1_list = []\n",
    "    vs_2_list = []\n",
    "    es_list = []\n",
    "    # Loop through batches\n",
    "    pbar = tqdm(range(num_batches))\n",
    "    for b in pbar:\n",
    "        start, end = b * batch_size, min((b + 1) * batch_size, returns_all['prediction'].shape[0])\n",
    "        if start == end:\n",
    "            print(\"SKipping\")\n",
    "            continue  # Skip empty batch\n",
    "    \n",
    "        samples = sampless[start:end, :, :, :]\n",
    "        target = targets[start:end, :, :]\n",
    "    \n",
    "        crps = np.mean(sr.crps_ensemble(target.detach().cpu(), samples.detach().cpu(), estimator='pwm'))\n",
    "        crps_sum = np.mean(sr.crps_ensemble(target.detach().cpu().sum(axis=-1), samples.detach().cpu().sum(axis=-2), estimator='pwm'))\n",
    "        # vs_05 = np.mean(sr.variogram_score(target.permute(0, 2, 1), samples.permute(0, 2, 3, 1), p=0.5, backend='numba'))\n",
    "        vs_05 = torch.mean(vs_ensemble_torch(target.permute(0, 2, 1).to(device), samples.permute(0, 2, 3, 1).to(device), p=0.5))\n",
    "        # vs_1 = np.mean(sr.variogram_score(target.permute(0, 2, 1), samples.permute(0, 2, 3, 1), p=1.0, backend='numba'))\n",
    "        vs_1 = torch.mean(vs_ensemble_torch(target.permute(0, 2, 1).to(device), samples.permute(0, 2, 3, 1).to(device), p=1))\n",
    "        # vs_2 = np.mean(sr.variogram_score(target.permute(0, 2, 1), samples.permute(0, 2, 3, 1), p=2.0, backend='numba'))\n",
    "        vs_2 = torch.mean(vs_ensemble_torch(target.permute(0, 2, 1).to(device), samples.permute(0, 2, 3, 1).to(device), p=2))\n",
    "        \n",
    "        # es = np.mean(sr.energy_score(target.permute(0, 2, 1), samples.permute(0, 2, 3, 1), backend='numba'))\n",
    "        es = torch.mean(es_ensemble_torch(target.permute(0, 2, 1).to(device), samples.permute(0, 2, 3, 1).to(device)))\n",
    "        \n",
    "        # Append to lists\n",
    "        crps_list.append(crps)\n",
    "        crps_sum_list.append(crps_sum)\n",
    "        vs_05_list.append(vs_05.detach().cpu())\n",
    "        vs_1_list.append(vs_1.detach().cpu())\n",
    "        vs_2_list.append(vs_2.detach().cpu())\n",
    "        es_list.append(es.detach().cpu())\n",
    "    \n",
    "    # Final averages\n",
    "    final_scores = {\n",
    "        \"CRPS\": np.mean(crps_list),\n",
    "        \"CRPS_Sum\": np.mean(crps_sum_list),\n",
    "        \"VS_0.5\": np.mean(vs_05_list),\n",
    "        \"VS_1.0\": np.mean(vs_1_list),\n",
    "        \"VS_2.0\": np.mean(vs_2_list),\n",
    "        \"ES\": np.mean(es_list),\n",
    "    }\n",
    "    return final_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce4aeff-3861-45cf-9eca-543aa5f87a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_all(configs):\n",
    "    eval_dict = {rs:{} for rs in configs.keys()}\n",
    "    for rs in configs.keys():\n",
    "        for key in configs[rs].keys():\n",
    "            samples = sample(configs[rs][key]['runner'], configs[rs][key]['returns_all'], random_state=rs)\n",
    "            eval_dict[rs][key] = evaluate(samples, configs[rs][key]['returns_all'], batch_size=4)\n",
    "    print(eval_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1882b782-9ae0-49ac-8cab-28bc47f3470f",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_all(configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86c945b-bced-4cf3-bd9e-115651f3ae4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {0: {'ETTh1_PTST_u': {'CRPS': np.float64(2.4197740580153275), 'CRPS_Sum': np.float64(8.704141144967366), 'VS_0.5': np.float32(572256.2), 'VS_1.0': np.float32(14644100.0), 'VS_2.0': np.float32(1985060100000.0), 'ES': np.float32(83.836754)}}, \n",
    "           1: {'ETTh1_PTST_u': {'CRPS': np.float64(2.4191808755737285), 'CRPS_Sum': np.float64(8.702462262138733), 'VS_0.5': np.float32(572201.0), 'VS_1.0': np.float32(14609265.0), 'VS_2.0': np.float32(28738159000.0), 'ES': np.float32(83.84088)}}, \n",
    "           2: {'ETTh1_PTST_u': {'CRPS': np.float64(2.4195399059040246), 'CRPS_Sum': np.float64(8.704312420450742), 'VS_0.5': np.float32(572196.56), 'VS_1.0': np.float32(14605370.0), 'VS_2.0': np.float32(48451195000.0), 'ES': np.float32(83.866234)}}, \n",
    "           3: {'ETTh1_PTST_u': {'CRPS': np.float64(2.41907135491505), 'CRPS_Sum': np.float64(8.702063031509304), 'VS_0.5': np.float32(572468.4), 'VS_1.0': np.float32(14621595.0), 'VS_2.0': np.float32(48899846000.0), 'ES': np.float32(83.85971)}}, \n",
    "           4: {'ETTh1_PTST_u': {'CRPS': np.float64(2.4195961071133296), 'CRPS_Sum': np.float64(8.704253091582524), 'VS_0.5': np.float32(572262.1), 'VS_1.0': np.float32(14613185.0), 'VS_2.0': np.float32(138508400000.0), 'ES': np.float32(83.84413)}}}\n",
    "# Extract metrics\n",
    "metrics = list(next(iter(results.values()))['ETTh1_PTST_u'].keys())\n",
    "agg = {metric: [] for metric in metrics}\n",
    "\n",
    "# rescaling = {\n",
    "#     \"VS_0.5\": 1e-4,\n",
    "#     \"VS_1.0\": 1e-6,\n",
    "#     \"VS_2.0\": 1e-10,\n",
    "# }\n",
    "rescaling = {}\n",
    "for run in results.values():\n",
    "    for metric in metrics:\n",
    "        if metric in rescaling.keys():\n",
    "            agg[metric].append(run['ETTh1_PTST_u'][metric]*rescaling[metric])\n",
    "        else:\n",
    "            agg[metric].append(run['ETTh1_PTST_u'][metric])\n",
    "\n",
    "# Compute stats\n",
    "summary = {}\n",
    "for metric in metrics:\n",
    "    values = np.array(agg[metric], dtype=np.float64)\n",
    "    summary[metric] = {\n",
    "        \"mean\": np.mean(values),\n",
    "        \"std\": np.std(values)\n",
    "    }\n",
    "\n",
    "# Display results\n",
    "for metric, stats in summary.items():\n",
    "    print(f\"{metric}: {stats['mean']:.4f} ± {stats['std']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91e8461-cfc7-45b7-b948-13ae5ba31129",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scoringrules as sr\n",
    "import numpy as np\n",
    "device = returns_all['target'].device\n",
    "targets = returns_all['target'].squeeze(-1)#.detach().cpu()\n",
    "sampless = prediction.permute(0, 2, 3, 1)#.detach().cpu() \n",
    "print(sampless.shape)\n",
    "print(targets.shape)\n",
    " # 3. Compute approximate metrics\n",
    "batch_size = 4\n",
    "num_batches = int(returns_all['prediction'].shape[0]/batch_size)+1\n",
    "# Lists to accumulate metric values\n",
    "crps_list = []\n",
    "crps_sum_list = []\n",
    "vs_05_list = []\n",
    "vs_1_list = []\n",
    "vs_2_list = []\n",
    "es_list = []\n",
    "\n",
    "import torch\n",
    "\n",
    "def vs_ensemble_torch(obs, fct, p=1.0):\n",
    "    \"\"\"\n",
    "    Compute Variogram Score using PyTorch on GPU.\n",
    "    obs: shape (..., D)\n",
    "    fct: shape (..., M, D)\n",
    "    \"\"\"\n",
    "    M = fct.shape[-2]\n",
    "\n",
    "    # Compute ensemble variogram component\n",
    "    fct_diff = fct.unsqueeze(-2) - fct.unsqueeze(-1)  # (B, M, D, D)\n",
    "    # print(fct_diff.shape)\n",
    "    vfct = (fct_diff.abs() ** p).sum(dim=-3) / M  # (B, D, D)\n",
    "    # print(vfct.shape)\n",
    "    # Compute observed variogram component\n",
    "    obs_diff = obs.unsqueeze(-2) - obs.unsqueeze(-1)  # (B, D, D)\n",
    "    vobs = (obs_diff.abs() ** p)  # (B, D, D)\n",
    "    # print(vobs.shape)\n",
    "    # print(vfct.shape)\n",
    "    vs = ((vfct - vobs) ** 2).sum(dim=(-2, -1))  # (B,)\n",
    "    return vs\n",
    "\n",
    "def es_ensemble_torch(obs: torch.Tensor, fct: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute the energy score using PyTorch.\n",
    "    \n",
    "    Parameters:\n",
    "    - obs: Tensor of shape (B, D)\n",
    "    - fct: Tensor of shape (B, M, D)\n",
    "\n",
    "    Returns:\n",
    "    - Tensor of shape (B,) with energy scores\n",
    "    \"\"\"\n",
    "    M = fct.shape[-2]\n",
    "\n",
    "    # E_1: mean norm between forecast samples and the observation\n",
    "    err_norm = torch.norm(fct - obs.unsqueeze(-2), dim=-1)  # (B, M)\n",
    "    E_1 = err_norm.sum(dim=-1) / M  # (B,)\n",
    "\n",
    "    # E_2: mean pairwise distance between forecast samples\n",
    "    spread = fct.unsqueeze(-3) - fct.unsqueeze(-2)  # (B, M, M, D)\n",
    "    spread_norm = torch.norm(spread, dim=-1)  # (B, M, M)\n",
    "    E_2 = spread_norm.sum(dim=(-2, -1)) / (M**2) # (B,)\n",
    "\n",
    "    return E_1 - 0.5 * E_2  # (B,)\n",
    "\n",
    "# Loop through batches\n",
    "pbar = tqdm(range(num_batches))\n",
    "for b in pbar:\n",
    "    start, end = b * batch_size, min((b + 1) * batch_size, returns_all['prediction'].shape[0])\n",
    "    if start == end:\n",
    "        print(\"SKipping\")\n",
    "        continue  # Skip empty batch\n",
    "\n",
    "    samples = sampless[start:end, :, :, :]\n",
    "    target = targets[start:end, :, :]\n",
    "\n",
    "    crps = np.mean(sr.crps_ensemble(target.detach().cpu(), samples.detach().cpu(), estimator='pwm'))\n",
    "    crps_sum = np.mean(sr.crps_ensemble(target.detach().cpu().sum(axis=-1), samples.detach().cpu().sum(axis=-2), estimator='pwm'))\n",
    "    # vs_05 = np.mean(sr.variogram_score(target.permute(0, 2, 1), samples.permute(0, 2, 3, 1), p=0.5, backend='numba'))\n",
    "    vs_05 = torch.mean(vs_ensemble_torch(target.permute(0, 2, 1).to(device), samples.permute(0, 2, 3, 1).to(device), p=0.5))\n",
    "    # vs_1 = np.mean(sr.variogram_score(target.permute(0, 2, 1), samples.permute(0, 2, 3, 1), p=1.0, backend='numba'))\n",
    "    vs_1 = torch.mean(vs_ensemble_torch(target.permute(0, 2, 1).to(device), samples.permute(0, 2, 3, 1).to(device), p=1))\n",
    "    # vs_2 = np.mean(sr.variogram_score(target.permute(0, 2, 1), samples.permute(0, 2, 3, 1), p=2.0, backend='numba'))\n",
    "    vs_2 = torch.mean(vs_ensemble_torch(target.permute(0, 2, 1).to(device), samples.permute(0, 2, 3, 1).to(device), p=2))\n",
    "    \n",
    "    # es = np.mean(sr.energy_score(target.permute(0, 2, 1), samples.permute(0, 2, 3, 1), backend='numba'))\n",
    "    es = torch.mean(es_ensemble_torch(target.permute(0, 2, 1).to(device), samples.permute(0, 2, 3, 1).to(device)))\n",
    "    \n",
    "    # Append to lists\n",
    "    crps_list.append(crps)\n",
    "    crps_sum_list.append(crps_sum)\n",
    "    vs_05_list.append(vs_05.detach().cpu())\n",
    "    vs_1_list.append(vs_1.detach().cpu())\n",
    "    vs_2_list.append(vs_2.detach().cpu())\n",
    "    es_list.append(es.detach().cpu())\n",
    "\n",
    "    # # Update tqdm with running averages\n",
    "    # # pbar.set_description(f\"CRPS: {np.mean(crps_list):.4f}, VS1: {np.mean(vs_1_list):.4f}, ES: {np.mean(es_list):.4f}\")\n",
    "    \n",
    "    # pbar.set_description(f\"VS: {vs_05:.4f}, VS_T: {vs_05_torch:.4f},\")\n",
    "\n",
    "# Final averages\n",
    "final_scores = {\n",
    "    \"CRPS\": np.mean(crps_list),\n",
    "    \"CRPS_Sum\": np.mean(crps_sum_list),\n",
    "    \"VS_0.5\": np.mean(vs_05_list),\n",
    "    \"VS_1.0\": np.mean(vs_1_list),\n",
    "    \"VS_2.0\": np.mean(vs_2_list),\n",
    "    \"ES\": np.mean(es_list),\n",
    "}\n",
    "\n",
    "print(\"\\nFinal Scores:\")\n",
    "for k, v in final_scores.items():\n",
    "    print(f\"{k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155d902f-88b6-4a96-b72a-c1fa01a87992",
   "metadata": {},
   "outputs": [],
   "source": [
    "Final Scores:\n",
    "CRPS: 2.4193\n",
    "CRPS_Sum: 8.7032\n",
    "VS_0.5: 572217.8750\n",
    "VS_1.0: 14610408.0000\n",
    "VS_2.0: 345045532672.0000\n",
    "ES: 83.8458\n",
    "\n",
    "\n",
    "Final Scores:\n",
    "CRPS: 2.4194\n",
    "CRPS_Sum: 8.7040\n",
    "VS_0.5: 572181.3125\n",
    "VS_1.0: 14610432.0000\n",
    "VS_2.0: 168409481216.0000\n",
    "ES: 83.8460"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (jupyter venv)",
   "language": "python",
   "name": "jupyter_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
